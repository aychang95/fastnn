{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Fast Neural Networks (FastNN)","text":"<p>A framework for deploying serializable and optimizable neural net models at scale in production via. the NVIDIA Triton Inference Server.</p> <p>"},{"location":"index.html#fastnn-docker-release-selector-ubuntu-1804","title":"FastNN Docker Release Selector (Ubuntu 18.04)","text":"<p>Info</p> Stable (0.2.0) Runtime Python 3.8 CUDA 10.2 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:0.2.0-cuda10.2-runtime-ubuntu18.04-py3.8</code></p> CUDA 11.6.0 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:0.2.0-cuda11.6.0-runtime-ubuntu20.04-py3.8</code></p> Development Python 3.8 CUDA 10.2 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:0.2.0-cuda10.2-devel-ubuntu18.04-py3.8</code></p> CUDA 11.6.0 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:0.2.0-cuda11.6.0-devel-ubuntu20.04-py3.8</code></p> Nightly Runtime Python 3.8 CUDA 10.2 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:latest-cuda10.2-runtime-ubuntu18.04-py3.8</code></p> CUDA 11.6.0 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:latest-cuda11.6.0-runtime-ubuntu20.04-py3.8</code></p> Development Python 3.8 CUDA 10.2 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:latest-cuda10.2-devel-ubuntu18.04-py3.8</code></p> CUDA 11.6.0 <p><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:latest-cuda11.6.0-devel-ubuntu20.04-py3.8</code></p>"},{"location":"index.html#documentation-guides-models-api-references","title":"Documentation - Guides, Models, API References","text":""},{"location":"index.html#features","title":"Features:","text":"<ul> <li>Data Processing<ul> <li>Intuitive data processing modules for encoding human-readible data into tensors compatible with deep learning libraries</li> </ul> </li> <li>Model Exporting<ul> <li>FastNN torch modules and tools for exporting models via. <code>TorchScript</code> tracing and scripting to a production environment</li> </ul> </li> <li>Model Zoo<ul> <li>Various exported models hosted in this repo via. git-lfs and AWS S3. Includes models from the HuggingFace's Transformers and  TorchVision</li> </ul> </li> <li>Model Deployment<ul> <li>Deploy models using Triton Inference Server on CPU/GPU-compatible server(s) with helm or docker</li> </ul> </li> <li>FastNN Client<ul> <li>Client wrapper for Triton Inference Server's client module for programmatic requests with python</li> </ul> </li> </ul>"},{"location":"index.html#pre-requisites","title":"Pre-Requisites:","text":"<p>Git LFS is required if you'd like to use any of the models provided by FastNN in <code>./model_repository</code>.</p> <p>Cloning this repository without Git LFS will clone a repository with LFS pointers, not the actual model.</p> <p>After the repository is cloned and Git LFS is installed, set up with <code>git lfs install</code>.</p> <p>Download specific models with:</p> <pre><code>git lfs pull --include=\"model_repository/&lt;path-to-model-dir&gt;\" --exclude=\"\"\n</code></pre> <p>Download ALL models with:</p> <pre><code>git lfs pull\n\n# Or\n#git lfs pull --include=\"*\" --exclude=\"\"\n</code></pre>"},{"location":"index.html#quickstart-and-installation","title":"Quickstart and Installation:","text":""},{"location":"index.html#pre-requisites_1","title":"Pre-requisites:","text":"<p>Requirements: Python 3.7+, PyTorch 1+, TorchVision 0.7+, Triton Client</p> <p>Optional: CUDA Compatible GPU, NVIDIA Drivers, cudnn (PyTorch pre-built wheels)</p> <ol> <li> <p>To install PyTorch with TorchVision, please refer to the installation instructions on their web page here.</p> </li> <li> <p>The tritonclient package wheels are not hosted on the public PyPI server. We need to add the address of NVIDA's private python package index to the environment. You can complete these steps and install the tritonclient package by running the following.</p> </li> </ol> <pre><code># If you cloned this repo, you can just uncomment and run the one line below\n#sh ./scripts/install_triton_client.\npip install nvidia-pyindex\npip install tritonclient[all]\n</code></pre>"},{"location":"index.html#install-via-pip","title":"Install via. pip","text":"<p>Once the above requirements are set, you can install fastnn with the command below:</p> <pre><code>pip install fastnn\n</code></pre> <p>If you are comfortable with the latest default stable releases of PyTorch you can skip step 1 in the pre-requisites and run <code>pip install fastnn[torch]</code> instead.</p>"},{"location":"index.html#install-from-source-with-poetry-for-development","title":"Install from source with Poetry for development","text":"<p>You will need to install poetry by referring to the installation instructions on their web page here.</p> <p>After cloning the repository, just run <code>poetry install</code> to install with the .lock file.</p> <p>Activate the virtual environment with the command below:</p> <pre><code>poetry shell\n</code></pre>"},{"location":"index.html#docker","title":"Docker","text":"<p>Official FastNN images are hosted on Docker Hub.</p> <p>Select FastNN package and image versions by referencing the documentation. Development and runtime environments are available.</p> <p>Jupyter lab and notebook servers are accessible with notebook examples and terminal access <code>localhost:8888</code> with every image.</p> <p>Run the latest FastNN image by running below:</p> <pre><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:latest\n</code></pre> <p>Run images with specific configurations as can see in the example command below:</p> <pre><code>docker run --gpus all --rm -it -p 8888:8888 aychang/fastnn:0.2.0-cuda11.6.0-runtime-ubuntu20.04-py3.7\n</code></pre>"},{"location":"index.html#triton-inference-server-local-or-kubernetes-cluster","title":"Triton Inference Server: Local or Kubernetes Cluster","text":""},{"location":"index.html#local-deployment","title":"Local Deployment","text":"<p>Requirements:   - Docker 19.03+</p> <p>GPU Inference Requirements:</p> <ul> <li> <p>NVIDIA CUDA-Compatible GPU</p> </li> <li> <p>NVIDIA Container Toolkit (nvidia-docker)</p> </li> </ul> <p>Local deployment of the Triton Server uses the EXPLICIT model control mode. Local models must be explicitly specified with the <code>--load-model</code>  argument in <code>./scripts/start_triton_local.sh</code></p> <pre><code>export MODEL_REPOSITORY=$(pwd)/model_repository\nsh ./scripts/start_triton_local.sh\n</code></pre>"},{"location":"index.html#helm-chart-install-in-kubernetes-cluster","title":"Helm Chart install in Kubernetes Cluster","text":"<p>Requirements: kubectl 1.17+, Helm 3+, Kubernetes 1.17+</p> <p>You can currently install the local FastNN helm chart with the following instuctions:</p> <pre><code>cd ./k8s\nhelm install fastnn .\nexport MODEL_REPOSITORY=$(pwd)/model_repository\n</code></pre> <p>Note: The current local helm chart installation deploys Triton using the NONE model control mode. All models available in the S3 Model Zoo will be deployed...good luck.  Deployed models and model control mode can be edited in the helm chart deployment configuration file.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"data_processing.html","title":"Data Processing: Human legible data to tensor inputs for torch and torchscript models","text":""},{"location":"data_processing.html#available-processors","title":"Available Processors","text":""},{"location":"data_processing.html#natural-language-processing","title":"Natural Language Processing:","text":"Task Class Process Input Format Process output Format SQuAD Question Answering <code>TransformersQAProcessor</code> <code>**{query: List[str], context: List[str]}</code> <code>List[torch.Tensor, torch.Tensor, torch.Tensor]</code> Token Classification (NER) <code>TransformersTokenTaggingProcessor</code> <code>**(text: List[str]</code> <code>List[List[Tuple[str,str]]]</code>"},{"location":"data_processing.html#computer-vision","title":"Computer Vision","text":"Task Class Process Input Format Process output Format Object Detection <code>ObjectDetectionProcessor</code> <code>str</code>(Path to directory of image files) <code>List[List[Tuple[torch.Tensor, np.array]]]</code>"},{"location":"data_processing.html#processor-examples","title":"Processor Examples","text":""},{"location":"data_processing.html#squad-question-answering-transformersqaprocessor","title":"SQuAD Question Answering - <code>TransformersQAProcessor</code>:","text":"<pre><code>from fastnn.processors.nlp.question_answering import TransformersQAProcessor\n\ncontext = [\"\"\"Albert Einstein was born at Ulm, in W\u00fcrttemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. \nLater, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics. \nIn 1901, the year he gained his diploma, he acquired Swiss citizenship and, as he was unable to find a teaching post, he accepted a position as technical assistant in the Swiss Patent Office. In 1905 he obtained his doctor\u2019s degree.\nDuring his stay at the Patent Office, and in his spare time, he produced much of his remarkable work and in 1908 he was appointed Privatdozent in Berne. In 1909 he became Professor Extraordinary at Zurich, in 1911 Professor of \nTheoretical Physics at Prague, returning to Zurich in the following year to fill a similar post. In 1914 he was appointed Director of the Kaiser Wilhelm Physical Institute and Professor in the University of Berlin. He became a \nGerman citizen in 1914 and remained in Berlin until 1933 when he renounced his citizenship for political reasons and emigrated to America to take the position of Professor of Theoretical Physics at Princeton. He became a United \nStates citizen in 1940 and retired from his post in 1945. After World War II, Einstein was a leading figure in the World Government Movement, he was offered the Presidency of the State of Israel, which he declined, and he \ncollaborated with Dr. Chaim Weizmann in establishing the Hebrew University of Jerusalem. Einstein always appeared to have a clear view of the problems of physics and the determination to solve them. He had a strategy of \nhis own and was able to visualize the main stages on the way to his goal. He regarded his major achievements as mere stepping-stones for the next advance. At the start of his scientific work, Einstein realized the \ninadequacies of Newtonian mechanics and his special theory of relativity stemmed from an attempt to reconcile the laws of mechanics with the laws of the electromagnetic field. He dealt with classical problems of \nstatistical mechanics and problems in which they were merged with quantum theory: this led to an explanation of the Brownian movement of molecules. He investigated the thermal properties of light with a low radiation\ndensity and his observations laid the foundation of the photon theory of light. In his early days in Berlin, Einstein postulated that the correct interpretation of the special theory of relativity must also furnish a\ntheory of gravitation and in 1916 he published his paper on the general theory of relativity. During this time he also contributed to the problems of the theory of radiation and statistical mechanics.\"\"\"]\n\nquery = [\"When was Einstein born?\"]\n\n# Specify tokenizer for encoding\n\nmodel_name_or_path = \"distilbert-base-cased-distilled-squad\"\n\nprocessor = TransformersQAProcessor(model_name_or_path=model_name_or_path)\n\nexamples, features, dataloader = processor.process_batch(query=query*8, context=context*8, mini_batch_size=8, use_gpu=True)\n</code></pre>"},{"location":"data_processing.html#token-tagging-nerpos-transformerstokentaggingprocessor","title":"Token Tagging (NER/POS) - TransformersTokenTaggingProcessor","text":"<pre><code>from fastnn.processors.nlp.token_tagging import TransformersTokenTaggingProcessor\n\n# Conll03 label names\nlabel_strings = [\n    \"O\",       # Outside of a named entity\n    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n    \"I-MISC\",  # Miscellaneous entity\n    \"B-PER\",   # Beginning of a person's name right after another person's name\n    \"I-PER\",   # Person's name\n    \"B-ORG\",   # Beginning of an organisation right after another organisation\n    \"I-ORG\",   # Organisation\n    \"B-LOC\",   # Beginning of a location right after another location\n    \"I-LOC\"    # Location\n]\n\nmodel_name_or_path = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n\nprocessor = TransformersTokenTaggingProcessor(model_name_or_path=model_name_or_path, label_strings=label_strings)\n\n# Use context string from above in QA example\ndataloader = processor.process_batch(text=context, mini_batch_size=4, use_gpu=True)\n</code></pre>"},{"location":"data_processing.html#object-detection-objectdetectionprocessor","title":"Object Detection - <code>ObjectDetectionProcessor</code>","text":"<pre><code>from fastnn.processors.cv.object_detection import ObjectDetectionProcessor\n\n# COCO dataset category names\nlabel_strings = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nprocessor = ObjectDetectionProcessor(label_strings=label_strings)\n\n# Replace \"img_dir_path\" with root directory of .png or .jpeg images\ndataloader = processor.process_batch(dir_path=\"./img_dir_path\", mini_batch_size=2, use_gpu=False)\n</code></pre>"},{"location":"inference_client.html","title":"Send Requests to Triton Inference Server with FastNN Client","text":""},{"location":"inference_client.html#examples","title":"Examples:","text":""},{"location":"inference_client.html#distilbert-squad-model","title":"\"distilbert-squad\" Model","text":"<pre><code>from fastnn.processors.nlp.question_answering import TransformersQAProcessor\n\ncontext = [\"\"\"Albert Einstein was born at Ulm, in W\u00fcrttemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. \nLater, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics. \nIn 1901, the year he gained his diploma, he acquired Swiss citizenship and, as he was unable to find a teaching post, he accepted a position as technical assistant in the Swiss Patent Office. In 1905 he obtained his doctor\u2019s degree.\nDuring his stay at the Patent Office, and in his spare time, he produced much of his remarkable work and in 1908 he was appointed Privatdozent in Berne. In 1909 he became Professor Extraordinary at Zurich, in 1911 Professor of \nTheoretical Physics at Prague, returning to Zurich in the following year to fill a similar post. In 1914 he was appointed Director of the Kaiser Wilhelm Physical Institute and Professor in the University of Berlin. He became a \nGerman citizen in 1914 and remained in Berlin until 1933 when he renounced his citizenship for political reasons and emigrated to America to take the position of Professor of Theoretical Physics at Princeton. He became a United \nStates citizen in 1940 and retired from his post in 1945. After World War II, Einstein was a leading figure in the World Government Movement, he was offered the Presidency of the State of Israel, which he declined, and he \ncollaborated with Dr. Chaim Weizmann in establishing the Hebrew University of Jerusalem. Einstein always appeared to have a clear view of the problems of physics and the determination to solve them. He had a strategy of \nhis own and was able to visualize the main stages on the way to his goal. He regarded his major achievements as mere stepping-stones for the next advance. At the start of his scientific work, Einstein realized the \ninadequacies of Newtonian mechanics and his special theory of relativity stemmed from an attempt to reconcile the laws of mechanics with the laws of the electromagnetic field. He dealt with classical problems of \nstatistical mechanics and problems in which they were merged with quantum theory: this led to an explanation of the Brownian movement of molecules. He investigated the thermal properties of light with a low radiation\ndensity and his observations laid the foundation of the photon theory of light. In his early days in Berlin, Einstein postulated that the correct interpretation of the special theory of relativity must also furnish a\ntheory of gravitation and in 1916 he published his paper on the general theory of relativity. During this time he also contributed to the problems of the theory of radiation and statistical mechanics.\"\"\"]\n\nquery = [\"When was Einstein born?\"]\n\n# Specify tokenizer for encoding\n\nmodel_name_or_path = \"distilbert-base-cased-distilled-squad\"\n\nprocessor = TransformersQAProcessor(model_name_or_path=model_name_or_path)\n\nexamples, features, dataloader = processor.process_batch(query=query*8, context=context*8, mini_batch_size=8, use_gpu=True)\n</code></pre> <pre><code>import torch\nimport numpy as np\n\nfrom fastnn.client import FastNNClient\n\nclient = FastNNClient(url=\"127.0.0.1:8001\", model_name=\"distilbert-squad\", model_version=\"1\", client_type=\"grpc\")\n#client = FastNNClient(url=\"127.0.0.1:8000\", model_name=\"distilbert-squad\", model_version=\"1\", client_type=\"http\")\n\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        response = client.request(batch) \n        start_logits = response.as_numpy('output__0')\n        start_logits = np.asarray(start_logits, dtype=np.float32)\n        end_logits = response.as_numpy('output__1')\n        end_logits = np.asarray(end_logits, dtype=np.float32)\n        example_indices = response.as_numpy('output__2')\n        example_indices = np.asarray(example_indices, dtype=np.int64)\n\n        output = (torch.from_numpy(start_logits), torch.from_numpy(end_logits), torch.from_numpy(example_indices))\n        all_outputs.append(output)\nall_outputs\n</code></pre> <pre><code>from fastnn.processors.cv.object_detection import ObjectDetectionProcessor\n\n# COCO dataset category names\nlabel_strings = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nprocessor = ObjectDetectionProcessor(label_strings=label_strings)\n\n# Replace \"img_dir_path\" with root directory of .png or .jpeg images\ndataloader = processor.process_batch(dir_path=\"./img_dir_path\", mini_batch_size=2, use_gpu=False)\n</code></pre>"},{"location":"inference_client.html#dbmdzbert-large-cased-finetuned-conll03-english","title":"\"dbmdz/bert-large-cased-finetuned-conll03-english\"","text":"<pre><code>from fastnn.nn.token_tagging import NERModule\nfrom fastnn.processors.nlp.token_tagging import TransformersTokenTaggingProcessor\nfrom fastnn.exporting import TorchScriptExporter\nfrom fastnn.client import FastNNClient\n</code></pre> <pre><code>context = [\"\"\"Albert Einstein was born at Ulm, in W\u00fcrttemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. Later, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics. In 1901, the year he gained his diploma, he acquired Swiss citizenship and, as he was unable to find a teaching post, he accepted a position as technical assistant in the Swiss Patent Office. In 1905 he obtained his doctor\u2019s degree.\n\nDuring his stay at the Patent Office, and in his spare time, he produced much of his remarkable work and in 1908 he was appointed Privatdozent in Berne. In 1909 he became Professor Extraordinary at Zurich, in 1911 Professor of Theoretical Physics at Prague, returning to Zurich in the following year to fill a similar post. In 1914 he was appointed Director of the Kaiser Wilhelm Physical Institute and Professor in the University of Berlin. He became a German citizen in 1914 and remained in Berlin until 1933 when he renounced his citizenship for political reasons and emigrated to America to take the position of Professor of Theoretical Physics at Princeton*. He became a United States citizen in 1940 and retired from his post in 1945.\n\nAfter World War II, Einstein was a leading figure in the World Government Movement, he was offered the Presidency of the State of Israel, which he declined, and he collaborated with Dr. Chaim Weizmann in establishing the Hebrew University of Jerusalem.\n\nEinstein always appeared to have a clear view of the problems of physics and the determination to solve them. He had a strategy of his own and was able to visualize the main stages on the way to his goal. He regarded his major achievements as mere stepping-stones for the next advance.\n\nAt the start of his scientific work, Einstein realized the inadequacies of Newtonian mechanics and his special theory of relativity stemmed from an attempt to reconcile the laws of mechanics with the laws of the electromagnetic field. He dealt with classical problems of statistical mechanics and problems in which they were merged with quantum theory: this led to an explanation of the Brownian movement of molecules. He investigated the thermal properties of light with a low radiation density and his observations laid the foundation of the photon theory of light.\nIn his early days in Berlin, Einstein postulated that the correct interpretation of the special theory of relativity must also furnish a theory of gravitation and in 1916 he published his paper on the general theory of relativity. During this time he also contributed to the problems of the theory of radiation and statistical mechanics.\"\"\",]\nmodel_name_or_path = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n\n\nlabel_strings = [\n    \"O\",       # Outside of a named entity\n    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n    \"I-MISC\",  # Miscellaneous entity\n    \"B-PER\",   # Beginning of a person's name right after another person's name\n    \"I-PER\",   # Person's name\n    \"B-ORG\",   # Beginning of an organisation right after another organisation\n    \"I-ORG\",   # Organisation\n    \"B-LOC\",   # Beginning of a location right after another location\n    \"I-LOC\"    # Location\n]\n\nprocessor = TransformersTokenTaggingProcessor(model_name_or_path, label_strings=label_strings)\n\ndataloader = processor.process_batch(context*2, mini_batch_size=2, use_gpu=False)\n</code></pre> <pre><code>client = FastNNClient(url=\"127.0.0.1:8001\", model_name=\"dbmdz.bert-large-cased-finetuned-conll03-english\", model_version=\"1\", client_type=\"grpc\")\n#client = FastNNClient(url=\"127.0.0.1:8000\", model_name=\"dbmdz.bert-large-cased-finetuned-conll03-english\", model_version=\"1\", client_type=\"http\")\n\nimport time\nimport torch\nimport numpy as np\n\nstart = time.time()\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        response = client.request(batch) \n        logits = response.as_numpy('output__0')\n        logits = np.asarray(logits, dtype=np.float32)\n        input_ids = response.as_numpy('output__1')\n        input_ids = np.asarray(input_ids, dtype=np.int64)\n\n        output = (torch.from_numpy(logits), torch.from_numpy(input_ids))\n        all_outputs.append(output)\nend = time.time()\nprint(end-start)\n</code></pre> <pre><code>results = processor.process_output_batch(all_outputs)\nresults\n</code></pre>"},{"location":"inference_client.html#fasterrcnn-resnet50-model","title":"\"fasterrcnn-resnet50\" Model","text":"<pre><code>import torch\nimport numpy as np\n\nfrom fastnn.client import FastNNClient\n\nclient = FastNNClient(url=\"127.0.0.1:8000\", model_name=\"fasterrcnn-resnet50-cpu\", model_version=\"1\", client_type=\"grpc\")\nclient = FastNNClient(url=\"127.0.0.1:8001\", model_name=\"fasterrcnn-resnet50-cpu\", model_version=\"1\", client_type=\"http\")\n\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        response = client.request(*batch) \n        boxes = response.as_numpy('output__0')\n        boxes = np.asarray(boxes, dtype=np.float32)\n        labels = response.as_numpy('output__1')\n        labels = np.asarray(labels, dtype=np.int64)\n        scores = response.as_numpy('output__2')\n        scores = np.asarray(scores, dtype=np.float32)\n\n        output = (torch.from_numpy(boxes), torch.from_numpy(labels), torch.from_numpy(scores))\n        all_outputs.append(output)\n\nall_outputs\n</code></pre>"},{"location":"model_deployment.html","title":"Deploy Models with Triton Inference Server","text":"<p>Pre-requisites:</p> <p>Clone the fastnn repository.</p> <p>Add NVIDIA's python package index and install FastNN/Triton Inference Server client</p> <p><code>sh ./scripts/install_triton_client.py</code></p>"},{"location":"model_deployment.html#local-deployment","title":"Local Deployment","text":"<p>Requirements:   - Docker 19.03+</p> <p>GPU Inference Requirements:</p> <ul> <li> <p>NVIDIA CUDA-Compatible GPU</p> </li> <li> <p>NVIDIA Container Toolkit (nvidia-docker)</p> </li> </ul> <p>Local deployment of the Triton Server uses the EXPLICIT model control mode. Local models must be explicitly specified with the <code>--load-model</code>  argument in <code>./scripts/start_triton_local.sh</code></p> <pre><code>export MODEL_REPOSITORY=$(pwd)/model_repository\nsh ./scripts/start_triton_local.sh\n</code></pre>"},{"location":"model_deployment.html#helm-chart-install-in-kubernetes-cluster","title":"Helm Chart install in Kubernetes Cluster","text":"<p>Requirements: kubectl 1.17+, Helm 3+, Kubernetes 1.17+</p> <p>You can currently install the local FastNN helm chart with the following instuctions:</p> <pre><code>cd ./k8s\nhelm install fastnn .\nexport MODEL_REPOSITORY=$(pwd)/model_repository\n</code></pre> <p>Note: The current local helm chart installation deploys Triton using the NONE model control mode. All models available in the S3 Model Zoo will be deployed...good luck.  Deployed models and model control mode can be edited in the helm chart deployment configuration file.</p>"},{"location":"model_exporting.html","title":"Export Pytorch Model to TorchScript Module","text":""},{"location":"model_exporting.html#available-modules-for-exporting","title":"Available Modules for Exporting","text":""},{"location":"model_exporting.html#natural-language-processing","title":"Natural Language Processing:","text":"Model Architecture Class Model Input Model Output Compatible Processors GPU Support Bert with Question Answering Head <code>BertQAModule</code> <code>*(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)</code> <code>List[torch.Tensor, torch.Tensor, torch.Tensor]</code> <code>TransformersQAProcessor</code> Transformers model with Token Classification Head <code>NERModule</code> <code>*(torch.Tensor, torch.Tensor)</code> <code>List[torch.Tensor, torch.Tensor]</code> <code>TransformersTokenTaggingProcessor</code>"},{"location":"model_exporting.html#computer-vision","title":"Computer Vision","text":"Model Architecture Class Model Input Model Output Compatible Processors GPU Support Faster R-CNN with Resnet-50 Backbone <code>FasterRCNNModule</code> <code>Union[torch.Tensor, List[torch.Tensor]</code> <code>Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], List[torch.Tensor]]</code> <code>ObjectDetectionProcessor</code>"},{"location":"model_exporting.html#fastnn-model-exporting-with-torchscriptexporter","title":"FastNN Model Exporting with <code>TorchScriptExporter</code>","text":""},{"location":"model_exporting.html#bartqamodule","title":"<code>BartQAModule</code>","text":"<p>Run pre-requisite data processing steps. This will create compatible input data required for tracing our models in the next step.</p> <p>See <code>TransformersQAProcessor</code> section in Data Processing documentation page for more information.</p> <pre><code>from fastnn.processors.nlp.question_answering import TransformersQAProcessor\n\ncontext = [\"\"\"Albert Einstein was born at Ulm, in W\u00fcrttemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. \nLater, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics. \nIn 1901, the year he gained his diploma, he acquired Swiss citizenship and, as he was unable to find a teaching post, he accepted a position as technical assistant in the Swiss Patent Office. In 1905 he obtained his doctor\u2019s degree.\nDuring his stay at the Patent Office, and in his spare time, he produced much of his remarkable work and in 1908 he was appointed Privatdozent in Berne. In 1909 he became Professor Extraordinary at Zurich, in 1911 Professor of \nTheoretical Physics at Prague, returning to Zurich in the following year to fill a similar post. In 1914 he was appointed Director of the Kaiser Wilhelm Physical Institute and Professor in the University of Berlin. He became a \nGerman citizen in 1914 and remained in Berlin until 1933 when he renounced his citizenship for political reasons and emigrated to America to take the position of Professor of Theoretical Physics at Princeton*. He became a United \nStates citizen in 1940 and retired from his post in 1945. After World War II, Einstein was a leading figure in the World Government Movement, he was offered the Presidency of the State of Israel, which he declined, and he \ncollaborated with Dr. Chaim Weizmann in establishing the Hebrew University of Jerusalem. Einstein always appeared to have a clear view of the problems of physics and the determination to solve them. He had a strategy of \nhis own and was able to visualize the main stages on the way to his goal. He regarded his major achievements as mere stepping-stones for the next advance. At the start of his scientific work, Einstein realized the \ninadequacies of Newtonian mechanics and his special theory of relativity stemmed from an attempt to reconcile the laws of mechanics with the laws of the electromagnetic field. He dealt with classical problems of \nstatistical mechanics and problems in which they were merged with quantum theory: this led to an explanation of the Brownian movement of molecules. He investigated the thermal properties of light with a low radiation\ndensity and his observations laid the foundation of the photon theory of light. In his early days in Berlin, Einstein postulated that the correct interpretation of the special theory of relativity must also furnish a\ntheory of gravitation and in 1916 he published his paper on the general theory of relativity. During this time he also contributed to the problems of the theory of radiation and statistical mechanics.\"\"\"]\n\nquery = [\"When was Einstein born?\"]\n\nmodel_name_or_path = \"distilbert-base-cased-distilled-squad\"\n\nprocessor = TransformersQAProcessor(model_name_or_path=model_name_or_path)\n\nexamples, features, dataloader = processor.process_batch(query=query*8, context=context*8, mini_batch_size=8, use_gpu=True)\n</code></pre> <p>Using processed input data, export your torch model to a model compatible with C++ programs.</p> <pre><code>from fastnn.nn.question_answering import BertQAModule\nfrom fastnn.exporting import TorchScriptExporter\n\npytorch_model = BertQAModule(model_name_or_path=model_name_or_path)\nexporter = TorchScriptExporter(model=pytorch_model, dataloader=dataloader, use_gpu=True)\ntorchscript_model = exporter.export()\nexporter.serialize(\"model.pt\")\n</code></pre>"},{"location":"model_exporting.html#nermodule","title":"NERModule","text":"<pre><code>from fastnn.processors.nlp.token_tagging import TransformersTokenTaggingProcessor\n\n# Conll03 label names\nlabel_strings = [\n    \"O\",       # Outside of a named entity\n    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n    \"I-MISC\",  # Miscellaneous entity\n    \"B-PER\",   # Beginning of a person's name right after another person's name\n    \"I-PER\",   # Person's name\n    \"B-ORG\",   # Beginning of an organisation right after another organisation\n    \"I-ORG\",   # Organisation\n    \"B-LOC\",   # Beginning of a location right after another location\n    \"I-LOC\"    # Location\n]\n\nmodel_name_or_path = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n\nprocessor = TransformersTokenTaggingProcessor(model_name_or_path=model_name_or_path, label_strings=label_strings)\n\n# Use context string from above in QA example\ndataloader = processor.process_batch(text=context, mini_batch_size=4, use_gpu=True)\n</code></pre> <pre><code>from fastnn.nn.token_tagging import NERModule\nfrom fastnn.exporting import TorchScriptExporter\n\npytorch_model = NERModule(model_name_or_path=model_name_or_path)\nexporter = TorchScriptExporter(model=pytorch_model, dataloader=dataloader, use_gpu=True)\ntorchscript_model = exporter.export()\nexporter.serialize(\"model.pt\")\n</code></pre>"},{"location":"model_exporting.html#fasterrcnnmodule","title":"<code>FasterRCNNModule</code>","text":"<pre><code>from fastnn.processors.cv.object_detection import ObjectDetectionProcessor\n\n# COCO dataset category names\nlabel_strings = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nprocessor = ObjectDetectionProcessor(label_strings=label_strings)\n\n# Replace \"img_dir_path\" with root directory of .png or .jpeg images\ndataloader = processor.process_batch(dir_path=\"./img_data_dir\", mini_batch_size=2, use_gpu=False)\n</code></pre> <pre><code>from fastnn.nn.object_detection import FasterRCNNModule\nfrom fastnn.exporting import TorchScriptExporter\n\npytorch_model = FasterRCNNModule() \nexporter = TorchScriptExporter(model=pytorch_model, dataloader=dataloader, use_gpu=False)\nexported_model = exporter.export() # May have to run this twice to force\nexporter.serialize(\"model.pt\")\n\n# Triton Compatible Traced Model\nimport torch\ntriton_batch_tensor = next(iter(dataloader))[0][0]\nexported_model = torch.jit.trace(exporter.model, (triton_batch_tensor))\nexported_model.save(\"model.pt\")\n</code></pre>"},{"location":"model_zoo.html","title":"Model Zoo","text":""},{"location":"model_zoo.html#available-serialized-models","title":"Available Serialized Models","text":""},{"location":"model_zoo.html#natural-language-processing","title":"Natural Language Processing:","text":"Model Name Size Origin distilbert-squad 249 MB Transformers distilbert-squad-cpu 249 MB Transformers bert-large-cased-whole-word-masking-finetuned-squad 1.24 GB Transformers bert-large-cased-whole-word-masking-finetuned-squad-cpu 1.24 GB Transformers deepset.roberta-base-squad2 474 MB Transformers deepset.roberta-base-squad2-cpu 474 MB Transformers deepset.bert-large-uncased-whole-word-masking-squad2 1.25 GB Transformers deepset.bert-large-uncased-whole-word-masking-squad2-cpu 1.25 GB Transformers mrm8488.bert-base-portuguese-cased-finetuned-squad-v1-pt 414 MB Transformers mrm8488.bert-base-portuguese-cased-finetuned-squad-v1-pt-cpu 414 MB Transformers"},{"location":"model_zoo.html#computer-vision","title":"Computer Vision","text":"Model Name Size Origin fasterrcnn-resnet50-cpu 160 MB TorchVision"},{"location":"model_zoo.html#run-inference-programatically-on-models-using-processor-and-fastnn-modules","title":"Run Inference Programatically on Models using <code>Processor</code> and FastNN Modules","text":""},{"location":"model_zoo.html#examples","title":"Examples:","text":""},{"location":"model_zoo.html#squad-question-answering","title":"SQuAD Question Answering","text":"<p>Run pre-requisite data processing steps. This will create compatible input data required for tracing our models in the next step.</p> <p>See <code>TransformersQAProcessor</code> section in Data Processing documentation page for more information.</p> <pre><code>from fastnn.processors.nlp.question_answering import TransformersQAProcessor\n\ncontext = [\"\"\"Albert Einstein was born at Ulm, in W\u00fcrttemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. \nLater, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics. \nIn 1901, the year he gained his diploma, he acquired Swiss citizenship and, as he was unable to find a teaching post, he accepted a position as technical assistant in the Swiss Patent Office. In 1905 he obtained his doctor\u2019s degree.\nDuring his stay at the Patent Office, and in his spare time, he produced much of his remarkable work and in 1908 he was appointed Privatdozent in Berne. In 1909 he became Professor Extraordinary at Zurich, in 1911 Professor of \nTheoretical Physics at Prague, returning to Zurich in the following year to fill a similar post. In 1914 he was appointed Director of the Kaiser Wilhelm Physical Institute and Professor in the University of Berlin. He became a \nGerman citizen in 1914 and remained in Berlin until 1933 when he renounced his citizenship for political reasons and emigrated to America to take the position of Professor of Theoretical Physics at Princeton*. He became a United \nStates citizen in 1940 and retired from his post in 1945. After World War II, Einstein was a leading figure in the World Government Movement, he was offered the Presidency of the State of Israel, which he declined, and he \ncollaborated with Dr. Chaim Weizmann in establishing the Hebrew University of Jerusalem. Einstein always appeared to have a clear view of the problems of physics and the determination to solve them. He had a strategy of \nhis own and was able to visualize the main stages on the way to his goal. He regarded his major achievements as mere stepping-stones for the next advance. At the start of his scientific work, Einstein realized the \ninadequacies of Newtonian mechanics and his special theory of relativity stemmed from an attempt to reconcile the laws of mechanics with the laws of the electromagnetic field. He dealt with classical problems of \nstatistical mechanics and problems in which they were merged with quantum theory: this led to an explanation of the Brownian movement of molecules. He investigated the thermal properties of light with a low radiation\ndensity and his observations laid the foundation of the photon theory of light. In his early days in Berlin, Einstein postulated that the correct interpretation of the special theory of relativity must also furnish a\ntheory of gravitation and in 1916 he published his paper on the general theory of relativity. During this time he also contributed to the problems of the theory of radiation and statistical mechanics.\"\"\"]\n\nquery = [\"When was Einstein born?\"]\n\nmodel_name_or_path = \"distilbert-base-cased-distilled-squad\"\n\nprocessor = TransformersQAProcessor(model_name_or_path=model_name_or_path)\n\nexamples, features, dataloader = processor.process_batch(query=query*8, context=context*8, mini_batch_size=8, use_gpu=True)\n</code></pre> <p>Using processed input data, export your torch model to a model compatible with C++ programs.</p> <p>See <code>BertQAModule</code> section in Exporting Models documentation page for more information.</p> <p>We'll show an example of running inference on a model from the \"model_repository\".</p> <pre><code>from fastnn.nn.question_answering import BertQAModule\n\npytorch_model = BertQAModule(model_name_or_path=model_name_or_path).to(\"cuda\")\npytorch_model.eval()\n</code></pre> <p>Run inference on loaded torchscript model and pytorch model.</p> <pre><code>import torch\n\ntorchscript_model = torch.jit.load(\"../../model_repository/distilbert-squad/1/model.pt\")\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        output = torchscript_model(*batch) \n        all_outputs.append(output)\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        output = pytorch_model(*batch) \n        all_outputs.append(output)\n</code></pre> <p>Use Processor class to produce human legible data from model outputs.</p> <pre><code>answers, n_best = processor.process_output_batch(\n        outputs = all_outputs,\n        examples = examples,\n        features = features,\n        n_best_size= 5,\n        max_answer_length= 16,\n        do_lower_case = False,\n        verbose_logging= False,\n        version_2_with_negative= False,\n        null_score_diff_threshold = 0.0\n)\n\nanswers, n_best\n</code></pre>"},{"location":"model_zoo.html#object-detection","title":"Object Detection","text":""},{"location":"model_zoo.html#fasterrcnnmodule","title":"<code>FasterRCNNModule</code>","text":"<pre><code>from fastnn.processors.cv.object_detection import ObjectDetectionProcessor\n\n# COCO dataset category names\nlabel_strings = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\nprocessor = ObjectDetectionProcessor(label_strings=label_strings)\n\n# Replace \"img_dir_path\" with root directory of .png or .jpeg images\ndataloader = processor.process_batch(dir_path=\"./img_dir_path\", mini_batch_size=2, use_gpu=False)\n</code></pre> <pre><code>from fastnn.nn.object_detection import FasterRCNNModule\n\npytorch_model = FasterRCNNModule().to(\"cpu\")\npytorch_model.eval()\n</code></pre> <p>Run inference on the loaded torchscript model as well as the pytorch model. Dataloader batch size for the torchscript model must be 1.</p> <pre><code>import torch\n\nloaded_torchscript_model = torch.jit.load(\"../../model_repository/fasterrcnn-resnet50-cpu/1/model.pt\")\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        output = pytorch_model(*batch) \n        all_outputs.append(output)\n\n#%%timeit\nall_outputs = []\nwith torch.no_grad():\n    for batch in dataloader:\n        output = loaded_torchscript_model(batch[0][0]) \n        all_outputs.append(output)\n</code></pre> <pre><code># Replace \"img_dir_path\" with root directory of .png or .jpeg images\ndataset = processor.process(dir_path=\"./img_dir_path\")\nresults = processor.process_output_batch(outputs=all_outputs, dataset=dataset)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,20))\nplt.imshow(results[1][0][1])\n</code></pre>"},{"location":"api-reference/exporter.html","title":"Exporter","text":""},{"location":"api-reference/exporter.html#torchscriptexporter","title":"<code>TorchScriptExporter</code>","text":"<p>Exporter Class for turning torch models into torch script models. Requires a torch Module and corresponding <code>DataLoader</code> populated with complete data samples. This means the <code>DataLoader</code> has atleast one full batch sample that can be used as input to the torch Module in order to run <code>torch.jit.trace()</code></p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; exporter = TorchScriptExporter(model=BertQAModule(...), dataloader=dataloader, use_gpu=True)\n&gt;&gt;&gt; exporter.export()\n</code></pre> <p>Parameters:</p> <ul> <li>model - Torch model/module, preferably from <code>fastnn.nn</code></li> <li>dataloader - Torch <code>DataLoader</code> that corresponds to the model</li> <li>use_gpu - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu</li> </ul> Source code in <code>fastnn/exporting.py</code> <pre><code>class TorchScriptExporter:\n    \"\"\"Exporter Class for turning torch models into torch script models. Requires a torch Module and corresponding `DataLoader` populated with complete data samples.\n    This means the `DataLoader` has atleast one full batch sample that can be used as input to the torch Module in order to run `torch.jit.trace()`\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; exporter = TorchScriptExporter(model=BertQAModule(...), dataloader=dataloader, use_gpu=True)\n    &gt;&gt;&gt; exporter.export()\n    ```\n\n    **Parameters:**\n\n    * **model** - Torch model/module, preferably from `fastnn.nn`\n    * **dataloader** - Torch `DataLoader` that corresponds to the model\n    * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, dataloader: DataLoader, use_gpu: bool = False\n    ):\n        if use_gpu:\n            if torch.cuda.is_available():\n                device = torch.device(\"cuda\")\n                logger.info(f\"Torch model set to device {device}\")\n            else:\n                device = torch.device(\"cpu\")\n                logger.info(f\"GPU not available...device set to {device}\")\n        else:\n            device = torch.device(\"cpu\")\n            logger.info(f\"Torch model set to device {device}\")\n\n        self.model = model\n        self.model.eval()\n        self.model.to(device)\n        self.dataloader = dataloader\n        self.torchscript_model = None\n\n    def export(self) -&gt; torch.jit.ScriptModule:\n        \"\"\"Traces pytorch model and returns `ScriptModule` model\"\"\"\n        batch_input = next(iter(self.dataloader))\n        self.torchscript_model = torch.jit.trace(self.model, tuple(batch_input))\n        return self.torchscript_model\n\n    def serialize(self, file_path: Union[Path, str]):\n        \"\"\"Serialize and save model\n\n        * **file_path** - String file path to save serialized torchscript model\n        \"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n\n        if self.torchscript_model:\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            self.torchscript_model.save(str(file_path))\n</code></pre>"},{"location":"api-reference/exporter.html#fastnn.exporting.TorchScriptExporter.export","title":"<code>export(self)</code>","text":"<p>Traces pytorch model and returns <code>ScriptModule</code> model</p> Source code in <code>fastnn/exporting.py</code> <pre><code>def export(self) -&gt; torch.jit.ScriptModule:\n    \"\"\"Traces pytorch model and returns `ScriptModule` model\"\"\"\n    batch_input = next(iter(self.dataloader))\n    self.torchscript_model = torch.jit.trace(self.model, tuple(batch_input))\n    return self.torchscript_model\n</code></pre>"},{"location":"api-reference/exporter.html#fastnn.exporting.TorchScriptExporter.serialize","title":"<code>serialize(self, file_path)</code>","text":"<p>Serialize and save model</p> <ul> <li>file_path - String file path to save serialized torchscript model</li> </ul> Source code in <code>fastnn/exporting.py</code> <pre><code>def serialize(self, file_path: Union[Path, str]):\n    \"\"\"Serialize and save model\n\n    * **file_path** - String file path to save serialized torchscript model\n    \"\"\"\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    if self.torchscript_model:\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        self.torchscript_model.save(str(file_path))\n</code></pre>"},{"location":"api-reference/fastnn_client.html","title":"FastNN Client","text":""},{"location":"api-reference/fastnn_client.html#fastnnclient","title":"<code>FastNNClient</code>","text":"<p>FastNN client class for pushing requests to the Triton Inference Server</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; client = FastNNClient(url=\"127.0.0.1:8000\", model_name=\"distilbert-squad\", model_version=\"1\")\n&gt;&gt;&gt; client.request(batch=batch)\n</code></pre> <p>Parameters:</p> <ul> <li>url - String url of Triton Inference Server. Defaults to 127.0.0.1:8000</li> <li>model_name - String name of model in <code>model_repository</code> directory</li> <li>model_version - String model version name</li> <li>client_type - A string for choosing between http and grpc protocol [\"grpc\", \"http\"] (Defaults to \"grpc\")</li> <li>verbose - Bool for log verbosity</li> </ul> Source code in <code>fastnn/client.py</code> <pre><code>class FastNNClient:\n    \"\"\"FastNN client class for pushing requests to the Triton Inference Server\n\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; client = FastNNClient(url=\"127.0.0.1:8000\", model_name=\"distilbert-squad\", model_version=\"1\")\n    &gt;&gt;&gt; client.request(batch=batch)\n    ```\n\n    **Parameters:**\n\n    * **url** - String url of Triton Inference Server. Defaults to 127.0.0.1:8000\n    * **model_name** - String name of model in `model_repository` directory\n    * **model_version** - String model version name\n    * **client_type** - A string for choosing between http and grpc protocol [\"grpc\", \"http\"] (Defaults to \"grpc\")\n    * **verbose** - Bool for log verbosity\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str = \"127.0.0.1:8000\",\n        model_name: str = \"distilbert-squad\",\n        model_version: str = \"1\",\n        client_type: str = \"grpc\",\n        verbose: bool = False,\n    ):\n\n        if client_type == \"grpc\":\n            self.client_pkg = grpc\n        elif client_type == \"http\":\n            self.client_pkg = http\n        else:\n            ValueError(\"Paramater 'client_type' must be either grpc or http\")\n\n        self.url = url\n        self.model_name = model_name\n        self.model_version = model_version\n        self.client_type = client_type\n\n        self.triton_client = self.client_pkg.InferenceServerClient(\n            url=url, verbose=verbose\n        )\n        self.model_metadata = self.triton_client.get_model_metadata(\n            model_name=model_name, model_version=model_version\n        )\n        self.model_config = self.triton_client.get_model_config(\n            model_name=model_name, model_version=model_version\n        )\n\n    def request(\n        self, batch: Tuple[torch.Tensor], binary_data: bool = False\n    ) -&gt; InferResult:\n        \"\"\"Runs a request with the `batch` input that can be generated from a FastNN `Processor`\n\n        * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n        \"\"\"\n        if self.client_type == \"grpc\":\n            return self.request_grpc(batch=batch)\n        elif self.client_type == \"http\":\n            return self.request_http(batch=batch)\n        else:\n            ValueError(\"Paramater 'client_type' must be either grpc or http\")\n\n    def request_http(\n        self, batch: Tuple[torch.Tensor], binary_data: bool = False\n    ) -&gt; InferResult:\n        \"\"\"Runs an http request with the `batch` input that can be generated from a FastNN `Processor`\n\n        * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n        \"\"\"\n        inputs_metadata = self.model_metadata[\"inputs\"]\n        outputs_metadata = self.model_metadata[\"outputs\"]\n\n        # Assert batch input matches triton model metadata\n        assert len(batch) == len(inputs_metadata)\n\n        inputs = []\n        for i, metadata in enumerate(inputs_metadata):\n            inp = self.client_pkg.InferInput(\n                metadata[\"name\"], tuple(batch[i].shape), metadata[\"datatype\"]\n            )\n            inp.set_data_from_numpy(batch[i].cpu().numpy(), binary_data=binary_data)\n            inputs.append(inp)\n\n        outputs = []\n        for i, metadata in enumerate(outputs_metadata):\n            out = self.client_pkg.InferRequestedOutput(\n                metadata[\"name\"], binary_data=binary_data\n            )\n            outputs.append(out)\n\n        response = self.triton_client.infer(\n            model_name=self.model_name,\n            model_version=self.model_version,\n            inputs=inputs,\n            outputs=outputs,\n        )\n\n        return response\n\n    def request_grpc(\n        self, batch: Tuple[torch.Tensor], binary_data: bool = False\n    ) -&gt; InferResult:\n        \"\"\"Runs a grpc request with the `batch` input that can be generated from a FastNN `Processor`\n\n        * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n        \"\"\"\n        inputs_metadata = self.model_metadata.inputs\n        outputs_metadata = self.model_metadata.outputs\n\n        # Assert batch input matches triton model metadata\n        assert len(batch) == len(inputs_metadata)\n\n        inputs = []\n        for i, metadata in enumerate(inputs_metadata):\n            inp = self.client_pkg.InferInput(\n                metadata.name, tuple(batch[i].shape), metadata.datatype\n            )\n            inp.set_data_from_numpy(batch[i].cpu().numpy())\n            inputs.append(inp)\n\n        outputs = []\n        for i, metadata in enumerate(outputs_metadata):\n            out = self.client_pkg.InferRequestedOutput(metadata.name)\n            outputs.append(out)\n\n        response = self.triton_client.infer(\n            model_name=self.model_name,\n            model_version=self.model_version,\n            inputs=inputs,\n            outputs=outputs,\n        )\n\n        return response\n</code></pre>"},{"location":"api-reference/fastnn_client.html#fastnn.client.FastNNClient.request","title":"<code>request(self, batch, binary_data=False)</code>","text":"<p>Runs a request with the <code>batch</code> input that can be generated from a FastNN <code>Processor</code></p> <ul> <li>batch - Tuple of torch tensors, typically batch inputs from a dataloader</li> </ul> Source code in <code>fastnn/client.py</code> <pre><code>def request(\n    self, batch: Tuple[torch.Tensor], binary_data: bool = False\n) -&gt; InferResult:\n    \"\"\"Runs a request with the `batch` input that can be generated from a FastNN `Processor`\n\n    * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n    \"\"\"\n    if self.client_type == \"grpc\":\n        return self.request_grpc(batch=batch)\n    elif self.client_type == \"http\":\n        return self.request_http(batch=batch)\n    else:\n        ValueError(\"Paramater 'client_type' must be either grpc or http\")\n</code></pre>"},{"location":"api-reference/fastnn_client.html#fastnn.client.FastNNClient.request_grpc","title":"<code>request_grpc(self, batch, binary_data=False)</code>","text":"<p>Runs a grpc request with the <code>batch</code> input that can be generated from a FastNN <code>Processor</code></p> <ul> <li>batch - Tuple of torch tensors, typically batch inputs from a dataloader</li> </ul> Source code in <code>fastnn/client.py</code> <pre><code>def request_grpc(\n    self, batch: Tuple[torch.Tensor], binary_data: bool = False\n) -&gt; InferResult:\n    \"\"\"Runs a grpc request with the `batch` input that can be generated from a FastNN `Processor`\n\n    * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n    \"\"\"\n    inputs_metadata = self.model_metadata.inputs\n    outputs_metadata = self.model_metadata.outputs\n\n    # Assert batch input matches triton model metadata\n    assert len(batch) == len(inputs_metadata)\n\n    inputs = []\n    for i, metadata in enumerate(inputs_metadata):\n        inp = self.client_pkg.InferInput(\n            metadata.name, tuple(batch[i].shape), metadata.datatype\n        )\n        inp.set_data_from_numpy(batch[i].cpu().numpy())\n        inputs.append(inp)\n\n    outputs = []\n    for i, metadata in enumerate(outputs_metadata):\n        out = self.client_pkg.InferRequestedOutput(metadata.name)\n        outputs.append(out)\n\n    response = self.triton_client.infer(\n        model_name=self.model_name,\n        model_version=self.model_version,\n        inputs=inputs,\n        outputs=outputs,\n    )\n\n    return response\n</code></pre>"},{"location":"api-reference/fastnn_client.html#fastnn.client.FastNNClient.request_http","title":"<code>request_http(self, batch, binary_data=False)</code>","text":"<p>Runs an http request with the <code>batch</code> input that can be generated from a FastNN <code>Processor</code></p> <ul> <li>batch - Tuple of torch tensors, typically batch inputs from a dataloader</li> </ul> Source code in <code>fastnn/client.py</code> <pre><code>def request_http(\n    self, batch: Tuple[torch.Tensor], binary_data: bool = False\n) -&gt; InferResult:\n    \"\"\"Runs an http request with the `batch` input that can be generated from a FastNN `Processor`\n\n    * **batch** - Tuple of torch tensors, typically batch inputs from a dataloader\n    \"\"\"\n    inputs_metadata = self.model_metadata[\"inputs\"]\n    outputs_metadata = self.model_metadata[\"outputs\"]\n\n    # Assert batch input matches triton model metadata\n    assert len(batch) == len(inputs_metadata)\n\n    inputs = []\n    for i, metadata in enumerate(inputs_metadata):\n        inp = self.client_pkg.InferInput(\n            metadata[\"name\"], tuple(batch[i].shape), metadata[\"datatype\"]\n        )\n        inp.set_data_from_numpy(batch[i].cpu().numpy(), binary_data=binary_data)\n        inputs.append(inp)\n\n    outputs = []\n    for i, metadata in enumerate(outputs_metadata):\n        out = self.client_pkg.InferRequestedOutput(\n            metadata[\"name\"], binary_data=binary_data\n        )\n        outputs.append(out)\n\n    response = self.triton_client.infer(\n        model_name=self.model_name,\n        model_version=self.model_version,\n        inputs=inputs,\n        outputs=outputs,\n    )\n\n    return response\n</code></pre>"},{"location":"api-reference/fastnn_modules.html","title":"FastNN Modules","text":""},{"location":"api-reference/fastnn_modules.html#question-answering-modules","title":"Question Answering Modules","text":"<p>Module for Bert-based Question Answering models from Transformers</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; model = BertQAModule(model_name_or_path=\"distilbert-base-cased-distilled-squad\")\n&gt;&gt;&gt; output = model(*batch)\n</code></pre> <p>Parameters:</p> <ul> <li>model_name_or_path - String key of Transformer's pre-trained model hosted in Hugging Face's Model Repository</li> </ul> Source code in <code>fastnn/nn/question_answering.py</code> <pre><code>class BertQAModule(torch.nn.Module):\n    \"\"\"Module for Bert-based Question Answering models from Transformers\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; model = BertQAModule(model_name_or_path=\"distilbert-base-cased-distilled-squad\")\n    &gt;&gt;&gt; output = model(*batch)\n    ```\n\n    **Parameters:**\n\n    * **model_name_or_path** - String key of Transformer's pre-trained model hosted in Hugging Face's Model Repository\n    \"\"\"\n\n    def __init__(\n        self, model_name_or_path: str = \"distilbert-base-cased-distilled-squad\"\n    ):\n        super(BertQAModule, self).__init__()\n        self.model = AutoModelForQuestionAnswering.from_pretrained(\n            model_name_or_path, torchscript=True\n        )\n\n    def forward(\n        self, input_ids, attention_mask, token_type_ids, example_indices, *args\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass on this QA module requires input_ids, attention_mask, and example_indices.\n        The `example_indices` parameter is required specifically for span-based extractive question anwering model outputs\n        in order to retain the structure of inputs within the final start and end logit calculations.\n\n        * **input_ids** - Tensor generated from FastNN `Processor` class\n        * **attention_mask** - Tensor generated from FastNN `Processor` class\n        * **token_type_ids** - Tensor generated from FastNN `Processor` class\n        * **example_indices** - Tensor generated from FastNN `Processor` class\n        * ** &amp;ast;args ** - args available to make abstraction easier for variant `Processor` classes\n        \"\"\"\n        start_logits, end_logits = self.model(\n            input_ids=input_ids, attention_mask=attention_mask\n        )\n        return start_logits, end_logits, example_indices\n</code></pre>"},{"location":"api-reference/fastnn_modules.html#fastnn.nn.question_answering.BertQAModule.forward","title":"<code>forward(self, input_ids, attention_mask, token_type_ids, example_indices, *args)</code>","text":"<p>Forward pass on this QA module requires input_ids, attention_mask, and example_indices. The <code>example_indices</code> parameter is required specifically for span-based extractive question anwering model outputs in order to retain the structure of inputs within the final start and end logit calculations.</p> <ul> <li>input_ids - Tensor generated from FastNN <code>Processor</code> class</li> <li>attention_mask - Tensor generated from FastNN <code>Processor</code> class</li> <li>token_type_ids - Tensor generated from FastNN <code>Processor</code> class</li> <li>example_indices - Tensor generated from FastNN <code>Processor</code> class</li> <li>** *args ** - args available to make abstraction easier for variant <code>Processor</code> classes</li> </ul> Source code in <code>fastnn/nn/question_answering.py</code> <pre><code>def forward(\n    self, input_ids, attention_mask, token_type_ids, example_indices, *args\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass on this QA module requires input_ids, attention_mask, and example_indices.\n    The `example_indices` parameter is required specifically for span-based extractive question anwering model outputs\n    in order to retain the structure of inputs within the final start and end logit calculations.\n\n    * **input_ids** - Tensor generated from FastNN `Processor` class\n    * **attention_mask** - Tensor generated from FastNN `Processor` class\n    * **token_type_ids** - Tensor generated from FastNN `Processor` class\n    * **example_indices** - Tensor generated from FastNN `Processor` class\n    * ** &amp;ast;args ** - args available to make abstraction easier for variant `Processor` classes\n    \"\"\"\n    start_logits, end_logits = self.model(\n        input_ids=input_ids, attention_mask=attention_mask\n    )\n    return start_logits, end_logits, example_indices\n</code></pre>"},{"location":"api-reference/fastnn_modules.html#token-tagging-modules","title":"Token Tagging Modules","text":"<p>Module for Token Classification models from Transformers</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; model = NERModule(model_name_or_path=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n&gt;&gt;&gt; output = model(*batch)\n</code></pre> <p>Parameters:</p> <ul> <li>model_name_or_path - String key of Transformer's pre-trained model hosted in Hugging Face's Model Repository</li> </ul> Source code in <code>fastnn/nn/token_tagging.py</code> <pre><code>class NERModule(torch.nn.Module):\n    \"\"\"Module for Token Classification models from Transformers\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; model = NERModule(model_name_or_path=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    &gt;&gt;&gt; output = model(*batch)\n    ```\n\n    **Parameters:**\n\n    * **model_name_or_path** - String key of Transformer's pre-trained model hosted in Hugging Face's Model Repository\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str = \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n    ):\n        super(NERModule, self).__init__()\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            model_name_or_path, torchscript=True\n        )\n\n    def forward(\n        self, input_ids, attention_mask\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass on this NER module requires input_ids and attention_mask.\n\n        * **input_ids** - Tensor generated from FastNN `Processor` class\n        * **attention_mask** - Tensor generated from FastNN `Processor` class\n        * ** &amp;ast;args ** - args available to make abstraction easier for variant `Processor` classes\n        \"\"\"\n        logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return logits[0], input_ids\n</code></pre>"},{"location":"api-reference/fastnn_modules.html#fastnn.nn.token_tagging.NERModule.forward","title":"<code>forward(self, input_ids, attention_mask)</code>","text":"<p>Forward pass on this NER module requires input_ids and attention_mask.</p> <ul> <li>input_ids - Tensor generated from FastNN <code>Processor</code> class</li> <li>attention_mask - Tensor generated from FastNN <code>Processor</code> class</li> <li>** *args ** - args available to make abstraction easier for variant <code>Processor</code> classes</li> </ul> Source code in <code>fastnn/nn/token_tagging.py</code> <pre><code>def forward(\n    self, input_ids, attention_mask\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass on this NER module requires input_ids and attention_mask.\n\n    * **input_ids** - Tensor generated from FastNN `Processor` class\n    * **attention_mask** - Tensor generated from FastNN `Processor` class\n    * ** &amp;ast;args ** - args available to make abstraction easier for variant `Processor` classes\n    \"\"\"\n    logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n    return logits[0], input_ids\n</code></pre>"},{"location":"api-reference/fastnn_modules.html#object-detection-modules","title":"Object Detection Modules","text":"<p>Module for Faster R-CNN Model with ResNet-50 backbone pre-trained on Coco dataset from PyTorch</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; model = FasterRCNNModule()\n&gt;&gt;&gt; output = model()\n</code></pre> <p>Parameters:</p> Source code in <code>fastnn/nn/object_detection.py</code> <pre><code>class FasterRCNNModule(torch.nn.Module):\n    \"\"\"Module for Faster R-CNN Model with ResNet-50 backbone pre-trained on Coco dataset from PyTorch\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; model = FasterRCNNModule()\n    &gt;&gt;&gt; output = model()\n    ```\n\n    **Parameters:**\n\n    \"\"\"\n\n    def __init__(self):\n        super(FasterRCNNModule, self).__init__()\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n\n    def forward(\n        self, images: Union[torch.Tensor, List[torch.Tensor]]\n    ) -&gt; Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], List[torch.Tensor]]:\n        \"\"\"Forward pass on this Module only requires a list of dynamic images of shape [C, H, W].\n        To work with a Triton Server which does not encode Python list objects, we enable a\n        `torch.Tensor` input that we can trace a model compatible with Triton inference server.\n\n        Both inputs are traceable with this model.\n\n        If input is list of `torch.Tensor` then the output will be a List of torch Tensors moddable by\n        3 where every 3 entries corresponds to one image.\n\n\n        * ** images ** - List of tensor images of shape [C, H, W]\n        \"\"\"\n        if isinstance(images, torch.Tensor):\n            predictions = self.model([images])\n            return (\n                predictions[0][\"boxes\"],\n                predictions[0][\"labels\"],\n                predictions[0][\"scores\"],\n            )\n        else:\n            predictions = self.model(images)\n            return self._dict2tensor(predictions)\n\n    def _dict2tensor(self, predictions):\n        prediction_outputs = []\n        for p in predictions:\n            if \"masks\" in p.keys():\n                prediction_outputs += [p[\"boxes\"], p[\"labels\"], p[\"scores\"], p[\"masks\"]]\n            prediction_outputs += [p[\"boxes\"], p[\"labels\"], p[\"scores\"]]\n        return prediction_outputs\n</code></pre>"},{"location":"api-reference/fastnn_modules.html#fastnn.nn.object_detection.FasterRCNNModule.forward","title":"<code>forward(self, images)</code>","text":"<p>Forward pass on this Module only requires a list of dynamic images of shape [C, H, W]. To work with a Triton Server which does not encode Python list objects, we enable a <code>torch.Tensor</code> input that we can trace a model compatible with Triton inference server.</p> <p>Both inputs are traceable with this model.</p> <p>If input is list of <code>torch.Tensor</code> then the output will be a List of torch Tensors moddable by 3 where every 3 entries corresponds to one image.</p> <ul> <li>** images ** - List of tensor images of shape [C, H, W]</li> </ul> Source code in <code>fastnn/nn/object_detection.py</code> <pre><code>def forward(\n    self, images: Union[torch.Tensor, List[torch.Tensor]]\n) -&gt; Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], List[torch.Tensor]]:\n    \"\"\"Forward pass on this Module only requires a list of dynamic images of shape [C, H, W].\n    To work with a Triton Server which does not encode Python list objects, we enable a\n    `torch.Tensor` input that we can trace a model compatible with Triton inference server.\n\n    Both inputs are traceable with this model.\n\n    If input is list of `torch.Tensor` then the output will be a List of torch Tensors moddable by\n    3 where every 3 entries corresponds to one image.\n\n\n    * ** images ** - List of tensor images of shape [C, H, W]\n    \"\"\"\n    if isinstance(images, torch.Tensor):\n        predictions = self.model([images])\n        return (\n            predictions[0][\"boxes\"],\n            predictions[0][\"labels\"],\n            predictions[0][\"scores\"],\n        )\n    else:\n        predictions = self.model(images)\n        return self._dict2tensor(predictions)\n</code></pre>"},{"location":"api-reference/processor.html","title":"Processor","text":""},{"location":"api-reference/processor.html#transformersqaprocessor","title":"<code>TransformersQAProcessor</code>","text":"<p>Question Answering Data Processor. Use this class to generate tensor inputs from human legible text/string data. This class can be used with a majority of the Bert architecture transformer models with the span-based extractive, Question Answering predictive head from Hugging Face.</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; processor = TRansformersQAProcessor(model_name_or_path=\"distilbert-base-cased-distilled-squad\")\n&gt;&gt;&gt; processor.process(query=[\"string\"], context[\"string\"])\n\n**Parameters:**\n\n* **model_name_or_path** - String defining HF question answering model/tokenizer's name\n</code></pre> Source code in <code>fastnn/processors/nlp/question_answering.py</code> <pre><code>class TransformersQAProcessor(Processor):\n    \"\"\"Question Answering Data Processor. Use this class to generate tensor inputs from human legible text/string data.\n    This class can be used with a majority of the Bert architecture transformer models with the span-based extractive,\n    Question Answering predictive head from Hugging Face.\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; processor = TRansformersQAProcessor(model_name_or_path=\"distilbert-base-cased-distilled-squad\")\n    &gt;&gt;&gt; processor.process(query=[\"string\"], context[\"string\"])\n\n    **Parameters:**\n\n    * **model_name_or_path** - String defining HF question answering model/tokenizer's name\n    ```\n    \"\"\"\n\n    def __init__(\n        self, model_name_or_path: str = \"distilbert-base-cased-distilled-squad\"\n    ):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_name_or_path, use_fast=False\n        )  # Can't use fast tokenizer yet for QA `use_fast=True`\n\n    def process(\n        self,\n        query: List[str],\n        context: List[str],\n        max_seq_length: int = 512,\n        doc_stride: int = 128,\n        max_query_length: int = 64,\n    ) -&gt; Tuple[List[SquadExample], List[SquadFeatures], Dataset]:\n        \"\"\"Generate torch `Dataset` object from query/context string pairs using specified tokenizer from HF.\n        This provides clear tensor input representations for compatible models.\n\n        Returns a tuple `Dataset` and matching `SquadFeatures`\n\n        * **query** - List of query strings, must be same length as `context`\n        * **context** - List of context strings, must be same length as `query`\n        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n        * **doc_stride** - Number of token strides to take when splitting up context into chunks of size `max_seq_length`\n        * **max_query_length** - Maximum token length for queries\n        \"\"\"\n        examples = self._generate_squad_examples(query=query, context=context)\n        features, dataset = squad_convert_examples_to_features(\n            examples,\n            self.tokenizer,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            is_training=False,\n            return_dataset=\"pt\",\n            threads=1,\n        )\n\n        return examples, features, dataset\n\n    def process_batch(\n        self,\n        query: List,\n        context: List,\n        mini_batch_size: int = 8,\n        max_seq_length: int = 512,\n        doc_stride: int = 128,\n        max_query_length: int = 64,\n        use_gpu: bool = False,\n    ) -&gt; Tuple[List[SquadExample], List[SquadFeatures], DataLoader]:\n        \"\"\"Generate torch `DataLoader` object from query/context string pairs using specified tokenizer from HF.\n        This provides clear tensor input representations for compatible models in an easy to use batch\n\n        Returns a tuple of (List[`SquadExample`], List[`SquadFeatures`], `DataLoader`)\n\n        * **query** - List of query strings, must be same length as `context`\n        * **context** - List of context strings, must be same length as `query`\n        * **mini_batch_size** - Batch size for inference\n        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n        * **doc_stride** - Number of token strides to take when splitting up context into chunks of size `max_seq_length`\n        * **max_query_length** - Maximum token length for queries\n        * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n        \"\"\"\n\n        if use_gpu:\n            if torch.cuda.is_available():\n                device = torch.device(\"cuda\")\n            else:\n                logger.info(\"GPU not available\")\n                device = torch.device(\"cpu\")\n        else:\n            device = torch.device(\"cpu\")\n\n        examples, features, dataset = self.process(\n            query=query,\n            context=context,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n        )\n\n        dataloader = DataLoader(\n            dataset,\n            batch_size=mini_batch_size,\n            collate_fn=lambda x: [t.to(device) for t in self._qa_collate_fn(x)],\n        )\n\n        return examples, features, dataloader\n\n    def process_output(\n        self,\n        outputs: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n        examples: List[SquadExample],\n        features: List[SquadFeatures],\n        n_best_size: int = 5,\n        max_answer_length: int = 10,\n        do_lower_case: bool = False,\n        verbose_logging: bool = False,\n        version_2_with_negative: bool = False,\n        null_score_diff_threshold: float = 0.0,\n    ):\n        pass\n\n    def process_output_batch(\n        self,\n        outputs: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n        examples: List[SquadExample],\n        features: List[SquadFeatures],\n        n_best_size: int = 5,\n        max_answer_length: int = 64,\n        do_lower_case: bool = False,\n        verbose_logging: bool = False,\n        version_2_with_negative: bool = False,\n        null_score_diff_threshold: float = 0.0,\n    ) -&gt; Tuple[OrderedDict, OrderedDict]:\n        \"\"\"Process output of Transformers QA model into human legible results.\n\n        * **outputs** - List of batch output tensors from a model's forward pass\n        * **examples** - List of `SquadExample` objects for each original context/query pair used as input. This is returned from the built-in `process()` or `process_batch()` methods\n        * **features** - List of `SquadFeature` objects for each context/query pair over the original doc_stride lengths. This is also returned from the built-in `process()` or `process_batch()` methods\n        * **n_best_size** - Number of top n results you want\n        * **max_answer_length** - Maximum token length for answers that are returned\n        * **do_lower_case** - Set as `True` if using uncased QA models\n        * **verbose_logging** - Set True if you want prediction verbose loggings\n        * **version_2_with_negative** - Set as True if using QA model with SQUAD2.0\n        * **null_score_diff_threshold** - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers\n        \"\"\"\n\n        # Generate results per example query\n        all_results: List[SquadResult] = []\n        for output in outputs:\n            example_indices = output[2]\n            for i, example_index in enumerate(example_indices):\n                start_logits = self._to_list(output[0][i])\n                end_logits = self._to_list(output[1][i])\n                eval_feature = features[example_index[0].item()]\n                unique_id = int(eval_feature.unique_id)\n                result = SquadResult(unique_id, start_logits, end_logits)\n                all_results.append(result)\n\n        # Compute predictions based off logits on a per example basis\n        answers, n_best = compute_predictions_logits(\n            all_examples=examples,\n            all_features=features,\n            all_results=all_results,\n            n_best_size=n_best_size,\n            max_answer_length=max_answer_length,\n            do_lower_case=do_lower_case,\n            verbose_logging=verbose_logging,\n            version_2_with_negative=version_2_with_negative,\n            null_score_diff_threshold=null_score_diff_threshold,\n            tokenizer=self.tokenizer,\n        )\n\n        return answers, n_best\n\n    def _qa_collate_fn(self, data):\n        batch = default_collate(data)\n        # Generate same batch dims for scalars to address future batch inferencing\n        batch[3].unsqueeze_(1)\n        batch[4].unsqueeze_(1)\n        return batch\n\n    def _to_list(self, tensor: torch.Tensor):\n        return tensor.detach().cpu().tolist()\n\n    def _generate_squad_examples(\n        self, query: List[str], context: List[str]\n    ) -&gt; List[SquadExample]:\n        \"\"\"Generate HF Squad Example objects with query/context pairs\"\"\"\n        assert len(query) == len(context)\n        examples = []\n        title = \"qa\"\n        is_impossible = False\n        answer_text = None\n        start_position_character = None\n        answers = [\"answer\"]\n        for idx, (q, c) in enumerate(zip(query, context)):\n            example = SquadExample(\n                qas_id=str(idx),\n                question_text=q,\n                context_text=c,\n                answer_text=answer_text,\n                start_position_character=start_position_character,\n                title=title,\n                is_impossible=is_impossible,\n                answers=answers,\n            )\n            examples.append(example)\n        return examples\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.question_answering.TransformersQAProcessor.process","title":"<code>process(self, query, context, max_seq_length=512, doc_stride=128, max_query_length=64)</code>","text":"<p>Generate torch <code>Dataset</code> object from query/context string pairs using specified tokenizer from HF. This provides clear tensor input representations for compatible models.</p> <p>Returns a tuple <code>Dataset</code> and matching <code>SquadFeatures</code></p> <ul> <li>query - List of query strings, must be same length as <code>context</code></li> <li>context - List of context strings, must be same length as <code>query</code></li> <li>max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with</li> <li>doc_stride - Number of token strides to take when splitting up context into chunks of size <code>max_seq_length</code></li> <li>max_query_length - Maximum token length for queries</li> </ul> Source code in <code>fastnn/processors/nlp/question_answering.py</code> <pre><code>def process(\n    self,\n    query: List[str],\n    context: List[str],\n    max_seq_length: int = 512,\n    doc_stride: int = 128,\n    max_query_length: int = 64,\n) -&gt; Tuple[List[SquadExample], List[SquadFeatures], Dataset]:\n    \"\"\"Generate torch `Dataset` object from query/context string pairs using specified tokenizer from HF.\n    This provides clear tensor input representations for compatible models.\n\n    Returns a tuple `Dataset` and matching `SquadFeatures`\n\n    * **query** - List of query strings, must be same length as `context`\n    * **context** - List of context strings, must be same length as `query`\n    * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n    * **doc_stride** - Number of token strides to take when splitting up context into chunks of size `max_seq_length`\n    * **max_query_length** - Maximum token length for queries\n    \"\"\"\n    examples = self._generate_squad_examples(query=query, context=context)\n    features, dataset = squad_convert_examples_to_features(\n        examples,\n        self.tokenizer,\n        max_seq_length=max_seq_length,\n        doc_stride=doc_stride,\n        max_query_length=max_query_length,\n        is_training=False,\n        return_dataset=\"pt\",\n        threads=1,\n    )\n\n    return examples, features, dataset\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.question_answering.TransformersQAProcessor.process_batch","title":"<code>process_batch(self, query, context, mini_batch_size=8, max_seq_length=512, doc_stride=128, max_query_length=64, use_gpu=False)</code>","text":"<p>Generate torch <code>DataLoader</code> object from query/context string pairs using specified tokenizer from HF. This provides clear tensor input representations for compatible models in an easy to use batch</p> <p>Returns a tuple of (List[<code>SquadExample</code>], List[<code>SquadFeatures</code>], <code>DataLoader</code>)</p> <ul> <li>query - List of query strings, must be same length as <code>context</code></li> <li>context - List of context strings, must be same length as <code>query</code></li> <li>mini_batch_size - Batch size for inference</li> <li>max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with</li> <li>doc_stride - Number of token strides to take when splitting up context into chunks of size <code>max_seq_length</code></li> <li>max_query_length - Maximum token length for queries</li> <li>use_gpu - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu</li> </ul> Source code in <code>fastnn/processors/nlp/question_answering.py</code> <pre><code>def process_batch(\n    self,\n    query: List,\n    context: List,\n    mini_batch_size: int = 8,\n    max_seq_length: int = 512,\n    doc_stride: int = 128,\n    max_query_length: int = 64,\n    use_gpu: bool = False,\n) -&gt; Tuple[List[SquadExample], List[SquadFeatures], DataLoader]:\n    \"\"\"Generate torch `DataLoader` object from query/context string pairs using specified tokenizer from HF.\n    This provides clear tensor input representations for compatible models in an easy to use batch\n\n    Returns a tuple of (List[`SquadExample`], List[`SquadFeatures`], `DataLoader`)\n\n    * **query** - List of query strings, must be same length as `context`\n    * **context** - List of context strings, must be same length as `query`\n    * **mini_batch_size** - Batch size for inference\n    * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n    * **doc_stride** - Number of token strides to take when splitting up context into chunks of size `max_seq_length`\n    * **max_query_length** - Maximum token length for queries\n    * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n    \"\"\"\n\n    if use_gpu:\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        else:\n            logger.info(\"GPU not available\")\n            device = torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    examples, features, dataset = self.process(\n        query=query,\n        context=context,\n        max_seq_length=max_seq_length,\n        doc_stride=doc_stride,\n        max_query_length=max_query_length,\n    )\n\n    dataloader = DataLoader(\n        dataset,\n        batch_size=mini_batch_size,\n        collate_fn=lambda x: [t.to(device) for t in self._qa_collate_fn(x)],\n    )\n\n    return examples, features, dataloader\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.question_answering.TransformersQAProcessor.process_output_batch","title":"<code>process_output_batch(self, outputs, examples, features, n_best_size=5, max_answer_length=64, do_lower_case=False, verbose_logging=False, version_2_with_negative=False, null_score_diff_threshold=0.0)</code>","text":"<p>Process output of Transformers QA model into human legible results.</p> <ul> <li>outputs - List of batch output tensors from a model's forward pass</li> <li>examples - List of <code>SquadExample</code> objects for each original context/query pair used as input. This is returned from the built-in <code>process()</code> or <code>process_batch()</code> methods</li> <li>features - List of <code>SquadFeature</code> objects for each context/query pair over the original doc_stride lengths. This is also returned from the built-in <code>process()</code> or <code>process_batch()</code> methods</li> <li>n_best_size - Number of top n results you want</li> <li>max_answer_length - Maximum token length for answers that are returned</li> <li>do_lower_case - Set as <code>True</code> if using uncased QA models</li> <li>verbose_logging - Set True if you want prediction verbose loggings</li> <li>version_2_with_negative - Set as True if using QA model with SQUAD2.0</li> <li>null_score_diff_threshold - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers</li> </ul> Source code in <code>fastnn/processors/nlp/question_answering.py</code> <pre><code>def process_output_batch(\n    self,\n    outputs: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n    examples: List[SquadExample],\n    features: List[SquadFeatures],\n    n_best_size: int = 5,\n    max_answer_length: int = 64,\n    do_lower_case: bool = False,\n    verbose_logging: bool = False,\n    version_2_with_negative: bool = False,\n    null_score_diff_threshold: float = 0.0,\n) -&gt; Tuple[OrderedDict, OrderedDict]:\n    \"\"\"Process output of Transformers QA model into human legible results.\n\n    * **outputs** - List of batch output tensors from a model's forward pass\n    * **examples** - List of `SquadExample` objects for each original context/query pair used as input. This is returned from the built-in `process()` or `process_batch()` methods\n    * **features** - List of `SquadFeature` objects for each context/query pair over the original doc_stride lengths. This is also returned from the built-in `process()` or `process_batch()` methods\n    * **n_best_size** - Number of top n results you want\n    * **max_answer_length** - Maximum token length for answers that are returned\n    * **do_lower_case** - Set as `True` if using uncased QA models\n    * **verbose_logging** - Set True if you want prediction verbose loggings\n    * **version_2_with_negative** - Set as True if using QA model with SQUAD2.0\n    * **null_score_diff_threshold** - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers\n    \"\"\"\n\n    # Generate results per example query\n    all_results: List[SquadResult] = []\n    for output in outputs:\n        example_indices = output[2]\n        for i, example_index in enumerate(example_indices):\n            start_logits = self._to_list(output[0][i])\n            end_logits = self._to_list(output[1][i])\n            eval_feature = features[example_index[0].item()]\n            unique_id = int(eval_feature.unique_id)\n            result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n\n    # Compute predictions based off logits on a per example basis\n    answers, n_best = compute_predictions_logits(\n        all_examples=examples,\n        all_features=features,\n        all_results=all_results,\n        n_best_size=n_best_size,\n        max_answer_length=max_answer_length,\n        do_lower_case=do_lower_case,\n        verbose_logging=verbose_logging,\n        version_2_with_negative=version_2_with_negative,\n        null_score_diff_threshold=null_score_diff_threshold,\n        tokenizer=self.tokenizer,\n    )\n\n    return answers, n_best\n</code></pre>"},{"location":"api-reference/processor.html#transformerstokentaggingprocessor","title":"<code>TransformersTokenTaggingProcessor</code>","text":"<p>Token Tagging Data Processor. Use this class to generate tensor inputs from human legible text/string data. This class can be used with a majority of the Bert architecture transformer models with a token-level predictive head for token classification from Hugging Face.</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; processor = TransformersTokenTaggingProcessor(model_name_or_path=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n&gt;&gt;&gt; processor.process(text=[\"string\"])\n\n**Parameters:**\n\n* **model_name_or_path** - String defining HF token tagging model/tokenizer's name\n* **label_strings** - List of strings that specify label strings with index as key for this specific processor\n</code></pre> Source code in <code>fastnn/processors/nlp/token_tagging.py</code> <pre><code>class TransformersTokenTaggingProcessor(Processor):\n    \"\"\"Token Tagging Data Processor. Use this class to generate tensor inputs from human legible text/string data.\n    This class can be used with a majority of the Bert architecture transformer models with a token-level predictive head\n    for token classification from Hugging Face.\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; processor = TransformersTokenTaggingProcessor(model_name_or_path=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    &gt;&gt;&gt; processor.process(text=[\"string\"])\n\n    **Parameters:**\n\n    * **model_name_or_path** - String defining HF token tagging model/tokenizer's name\n    * **label_strings** - List of strings that specify label strings with index as key for this specific processor\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str = \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n        label_strings: List[str] = [\n            \"O\",  # Outside of a named entity\n            \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n            \"I-MISC\",  # Miscellaneous entity\n            \"B-PER\",  # Beginning of a person's name right after another person's name\n            \"I-PER\",  # Person's name\n            \"B-ORG\",  # Beginning of an organisation right after another organisation\n            \"I-ORG\",  # Organisation\n            \"B-LOC\",  # Beginning of a location right after another location\n            \"I-LOC\",  # Location\n        ],\n    ):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_name_or_path, use_fast=True\n        )\n        self.label_strings = label_strings\n\n    def process(\n        self,\n        text: List[str],\n        max_seq_length: int = 512,\n    ) -&gt; Tuple:\n        \"\"\"Generate torch `Dataset` object from query/context string pairs using specified tokenizer from HF.\n        This provides clear tensor input representations for compatible models.\n\n        Returns a `Dataset`\n\n        * **text** - List of text strings\n        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n        \"\"\"\n\n        tokens = self.tokenizer(\n            text=text, return_tensors=\"pt\", padding=\"max_length\", truncation=True\n        )\n        dataset = TensorDataset(\n            tokens[\"input_ids\"],\n            tokens[\"attention_mask\"],\n        )\n\n        return dataset\n\n    def process_batch(\n        self,\n        text: List,\n        mini_batch_size: int = 8,\n        max_seq_length: int = 512,\n        use_gpu: bool = False,\n    ) -&gt; DataLoader:\n        \"\"\"Generate torch `DataLoader` object from text strings using specified tokenizer from HF.\n        This provides clear tensor input representations for compatible models in an easy to use batch\n\n        Returns a `DataLoader`\n\n        * **text** - List of text strings\n        * **mini_batch_size** - Batch size for inference\n        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n        * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n        \"\"\"\n\n        if use_gpu:\n            if torch.cuda.is_available():\n                device = torch.device(\"cuda\")\n            else:\n                logger.info(\"GPU not available\")\n                device = torch.device(\"cpu\")\n        else:\n            device = torch.device(\"cpu\")\n\n        dataset = self.process(text=text)\n\n        dataloader = DataLoader(\n            dataset,\n            batch_size=mini_batch_size,\n            collate_fn=lambda x: [t.to(device) for t in self._collate_fn(x)],\n        )\n\n        return dataloader\n\n    def process_output(\n        self,\n        outputs: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n    ):\n        pass\n\n    def process_output_batch(self, outputs: List) -&gt; List[List[Tuple[str, str]]]:\n        \"\"\"Process output of Transformers NER model\n\n        * **outputs** - List of batch output tensors from a model's forward pass\n        \"\"\"\n        results = []\n        for logits, input_ids in outputs:\n            tokens_batch = [self.tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n            argmax_batch = [torch.argmax(o, dim=1) for o in logits]\n            for i in range(len(tokens_batch)):\n                # Filter out padding\n                results.append(\n                    [\n                        (token, self.label_strings[prediction])\n                        for token, prediction in zip(\n                            tokens_batch[i], argmax_batch[i].cpu().numpy()\n                        )\n                        if token != \"[PAD]\"\n                    ]\n                )\n        return results\n\n    def _collate_fn(self, data):\n        batch = default_collate(data)\n        return batch\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.token_tagging.TransformersTokenTaggingProcessor.process","title":"<code>process(self, text, max_seq_length=512)</code>","text":"<p>Generate torch <code>Dataset</code> object from query/context string pairs using specified tokenizer from HF. This provides clear tensor input representations for compatible models.</p> <p>Returns a <code>Dataset</code></p> <ul> <li>text - List of text strings</li> <li>max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with</li> </ul> Source code in <code>fastnn/processors/nlp/token_tagging.py</code> <pre><code>def process(\n    self,\n    text: List[str],\n    max_seq_length: int = 512,\n) -&gt; Tuple:\n    \"\"\"Generate torch `Dataset` object from query/context string pairs using specified tokenizer from HF.\n    This provides clear tensor input representations for compatible models.\n\n    Returns a `Dataset`\n\n    * **text** - List of text strings\n    * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n    \"\"\"\n\n    tokens = self.tokenizer(\n        text=text, return_tensors=\"pt\", padding=\"max_length\", truncation=True\n    )\n    dataset = TensorDataset(\n        tokens[\"input_ids\"],\n        tokens[\"attention_mask\"],\n    )\n\n    return dataset\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.token_tagging.TransformersTokenTaggingProcessor.process_batch","title":"<code>process_batch(self, text, mini_batch_size=8, max_seq_length=512, use_gpu=False)</code>","text":"<p>Generate torch <code>DataLoader</code> object from text strings using specified tokenizer from HF. This provides clear tensor input representations for compatible models in an easy to use batch</p> <p>Returns a <code>DataLoader</code></p> <ul> <li>text - List of text strings</li> <li>mini_batch_size - Batch size for inference</li> <li>max_seq_length - Maximum context token length. Check model configs to see max sequence length the model was trained with</li> <li>use_gpu - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu</li> </ul> Source code in <code>fastnn/processors/nlp/token_tagging.py</code> <pre><code>def process_batch(\n    self,\n    text: List,\n    mini_batch_size: int = 8,\n    max_seq_length: int = 512,\n    use_gpu: bool = False,\n) -&gt; DataLoader:\n    \"\"\"Generate torch `DataLoader` object from text strings using specified tokenizer from HF.\n    This provides clear tensor input representations for compatible models in an easy to use batch\n\n    Returns a `DataLoader`\n\n    * **text** - List of text strings\n    * **mini_batch_size** - Batch size for inference\n    * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n    * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n    \"\"\"\n\n    if use_gpu:\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        else:\n            logger.info(\"GPU not available\")\n            device = torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    dataset = self.process(text=text)\n\n    dataloader = DataLoader(\n        dataset,\n        batch_size=mini_batch_size,\n        collate_fn=lambda x: [t.to(device) for t in self._collate_fn(x)],\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.nlp.token_tagging.TransformersTokenTaggingProcessor.process_output_batch","title":"<code>process_output_batch(self, outputs)</code>","text":"<p>Process output of Transformers NER model</p> <ul> <li>outputs - List of batch output tensors from a model's forward pass</li> </ul> Source code in <code>fastnn/processors/nlp/token_tagging.py</code> <pre><code>def process_output_batch(self, outputs: List) -&gt; List[List[Tuple[str, str]]]:\n    \"\"\"Process output of Transformers NER model\n\n    * **outputs** - List of batch output tensors from a model's forward pass\n    \"\"\"\n    results = []\n    for logits, input_ids in outputs:\n        tokens_batch = [self.tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n        argmax_batch = [torch.argmax(o, dim=1) for o in logits]\n        for i in range(len(tokens_batch)):\n            # Filter out padding\n            results.append(\n                [\n                    (token, self.label_strings[prediction])\n                    for token, prediction in zip(\n                        tokens_batch[i], argmax_batch[i].cpu().numpy()\n                    )\n                    if token != \"[PAD]\"\n                ]\n            )\n    return results\n</code></pre>"},{"location":"api-reference/processor.html#objectdetectionprocessor","title":"<code>ObjectDetectionProcessor</code>","text":"<p>Object Detection processor dealing with image files or 3xHxW formatted images and boxes, scores, labels out processing. Since most resizing and padding transforms are done by the object detection models in PyTorch, datasets and dataloaders willl generate batches of images as lists.</p> <p>Usage:</p> <pre><code>&gt;&gt;&gt; processor = ObjectDetectionProcessor()\n&gt;&gt;&gt; processor.process(file_paths=[\"file_path.png\"])\n\n**Parameters:**\n* **label_strings** - List of strings that specify label strings with index as key for this specific processor\n</code></pre> Source code in <code>fastnn/processors/cv/object_detection.py</code> <pre><code>class ObjectDetectionProcessor(Processor):\n    \"\"\"Object Detection processor dealing with image files or 3xHxW formatted images and boxes, scores, labels out processing.\n    Since most resizing and padding transforms are done by the object detection models in PyTorch, datasets and dataloaders willl\n    generate batches of images as lists.\n\n    Usage:\n    ```python\n    &gt;&gt;&gt; processor = ObjectDetectionProcessor()\n    &gt;&gt;&gt; processor.process(file_paths=[\"file_path.png\"])\n\n    **Parameters:**\n    * **label_strings** - List of strings that specify label strings with index as key for this specific processor\n\n    ```\n    \"\"\"\n\n    def __init__(self, label_strings: List[str]):\n        self.label_strings = label_strings\n\n    def process(\n        self,\n        dir_path: str,\n        transforms: Optional[Callable] = ConvertImageDtype(torch.float),\n    ) -&gt; Dataset:\n        \"\"\"Generate torch `Dataset` object from list of file paths or image Tensors.\n        This provides clear tensor input representations for compatible models.\n\n        Returns a Dataset\n\n        * **dir_path** - String path to directory of images you'd like to process\n        \"\"\"\n\n        dataset = ImageDataset(root=dir_path, transforms=transforms)\n\n        return dataset\n\n    def process_batch(\n        self,\n        dir_path: str,\n        transforms: Optional[Callable] = ConvertImageDtype(torch.float),\n        mini_batch_size: int = 8,\n        use_gpu: bool = False,\n    ) -&gt; DataLoader:\n        \"\"\"Generate torch `Dataloader` object from data directory path.\n        This provides clear tensor input representations for compatible models.\n\n        Returns a `Dataloader`\n\n        * **dir_path** - String path to directory of images you'd like to process\n        * **mini_batch_size** - Batch size for inference\n        * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n        \"\"\"\n\n        if use_gpu:\n            if torch.cuda.is_available():\n                device = torch.device(\"cuda\")\n            else:\n                logger.info(\"GPU not available\")\n                device = torch.device(\"cpu\")\n        else:\n            device = torch.device(\"cpu\")\n\n        dataset = self.process(dir_path=dir_path, transforms=transforms)\n\n        # Instead of a tensor batch, the lambda collate_fn will provide a list batch\n        dataloader = DataLoader(\n            dataset,\n            batch_size=mini_batch_size,\n            collate_fn=lambda x: [[t.to(device) for t in self._od_collate_fn(x)]],\n        )\n\n        return dataloader\n\n    def process_output(\n        self,\n    ):\n        pass\n\n    def process_output_batch(\n        self, outputs: List[List[torch.Tensor]], dataset: Dataset\n    ) -&gt; List[List[Tuple[torch.Tensor, np.array]]]:\n        \"\"\"Process output of object detection model into human legible results.\n        Outputs from `FasterRCNNModule`\n\n\n        Returns batched results of list of list of tuples containing boxed images in tensor and numpy format\n\n        * **outputs** - List of batch output tensors from a model's forward pass\n        * **dataset** - Corresponding dataset with originial images matched with model outputs\n\n        \"\"\"\n        # Labeled Images\n        results = []\n\n        for idx, out in enumerate(outputs):\n            labeled_images = []\n            for label_idx in range(1, len(out), 3):\n                labels = [self.label_strings[o] for o in out[label_idx]]\n\n                unique_labels = set(labels)\n                label_colors_map = {}\n                for label in unique_labels:\n                    label_colors_map[label] = tuple(\n                        np.random.choice(range(256), size=3)\n                    )\n\n                label_colors = [label_colors_map[label] for label in labels]\n\n                output_tensor, output_numpy = self.draw_bounding_boxes(\n                    ConvertImageDtype(torch.uint8)(\n                        dataset[idx * (len(out) // 3) + label_idx // 3]\n                    ),\n                    out[label_idx - 1],\n                    labels=labels,\n                    colors=label_colors,\n                )\n                labeled_images.append((output_tensor, output_numpy))\n            results.append(labeled_images)\n\n        return results\n\n    def _od_collate_fn(self, data):\n        \"\"\"Custom collate fn to output dynamic image batches without same-dim requirements via. `stack`.\n        This is not technically a \"correct\" collate_fn for most of torch's vision models. Should be wrapped as a list\n        in the lambda collate fn.\n        \"\"\"\n        data = [img for img in data]\n        return data\n\n    @torch.no_grad()\n    def draw_bounding_boxes(\n        self,\n        image: torch.Tensor,\n        boxes: torch.Tensor,\n        labels: Optional[List[str]] = None,\n        colors: Optional[List[Union[str, Tuple[int, int, int]]]] = None,\n        width: int = 1,\n        font: Optional[str] = \"arial.ttf\",\n        font_size: int = 10,\n    ) -&gt; Tuple[torch.Tensor, np.array]:\n\n        \"\"\"\n        Added and modified from TorchVision utils.\n        Draws bounding boxes on given image.\n        The values of the input image should be uint8 between 0 and 255.\n        Args:\n            image (Tensor): Tensor of shape (C x H x W)\n            bboxes (Tensor): Tensor of size (N, 4) containing bounding boxes in (xmin, ymin, xmax, ymax) format. Note that\n                the boxes are absolute coordinates with respect to the image. In other words: `0 &lt;= xmin &lt; xmax &lt; W` and\n                `0 &lt;= ymin &lt; ymax &lt; H`.\n            labels (List[str]): List containing the labels of bounding boxes.\n            colors (List[Union[str, Tuple[int, int, int]]]): List containing the colors of bounding boxes. The colors can\n                be represented as `str` or `Tuple[int, int, int]`.\n            width (int): Width of bounding box.\n            font (str): A filename containing a TrueType font. If the file is not found in this filename, the loader may\n                also search in other directories, such as the `fonts/` directory on Windows or `/Library/Fonts/`,\n                `/System/Library/Fonts/` and `~/Library/Fonts/` on macOS.\n            font_size (int): The requested font size in points.\n        \"\"\"\n\n        if not isinstance(image, torch.Tensor):\n            raise TypeError(f\"Tensor expected, got {type(image)}\")\n        elif image.dtype != torch.uint8:\n            raise ValueError(f\"Tensor uint8 expected, got {image.dtype}\")\n        elif image.dim() != 3:\n            raise ValueError(\"Pass individual images, not batches\")\n\n        ndarr = image.permute(1, 2, 0).numpy()\n        img_to_draw = Image.fromarray(ndarr)\n\n        img_boxes = boxes.to(torch.int64).tolist()\n\n        draw = ImageDraw.Draw(img_to_draw)\n\n        pixel_ratio = max(1, (max(ndarr.shape[0], ndarr.shape[1]) // 1000))\n\n        for i, bbox in enumerate(img_boxes):\n            color = None if colors is None else colors[i]\n            draw.rectangle(bbox, width=width * pixel_ratio, outline=color)\n\n            if labels is not None:\n                txt_font = (\n                    ImageFont.load_default()\n                    if font is None\n                    else ImageFont.truetype(font=font, size=font_size * pixel_ratio)\n                )\n                draw.text((bbox[0], bbox[1]), labels[i], fill=color, font=txt_font)\n\n        return torch.from_numpy(np.array(img_to_draw)).permute(2, 0, 1), np.array(\n            img_to_draw\n        )\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.cv.object_detection.ObjectDetectionProcessor.draw_bounding_boxes","title":"<code>draw_bounding_boxes(self, image, boxes, labels=None, colors=None, width=1, font='arial.ttf', font_size=10)</code>","text":"<p>Added and modified from TorchVision utils. Draws bounding boxes on given image. The values of the input image should be uint8 between 0 and 255.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor of shape (C x H x W)</p> required <code>bboxes</code> <code>Tensor</code> <p>Tensor of size (N, 4) containing bounding boxes in (xmin, ymin, xmax, ymax) format. Note that the boxes are absolute coordinates with respect to the image. In other words: <code>0 &lt;= xmin &lt; xmax &lt; W</code> and <code>0 &lt;= ymin &lt; ymax &lt; H</code>.</p> required <code>labels</code> <code>List[str]</code> <p>List containing the labels of bounding boxes.</p> <code>None</code> <code>colors</code> <code>List[Union[str, Tuple[int, int, int]]]</code> <p>List containing the colors of bounding boxes. The colors can be represented as <code>str</code> or <code>Tuple[int, int, int]</code>.</p> <code>None</code> <code>width</code> <code>int</code> <p>Width of bounding box.</p> <code>1</code> <code>font</code> <code>str</code> <p>A filename containing a TrueType font. If the file is not found in this filename, the loader may also search in other directories, such as the <code>fonts/</code> directory on Windows or <code>/Library/Fonts/</code>, <code>/System/Library/Fonts/</code> and <code>~/Library/Fonts/</code> on macOS.</p> <code>'arial.ttf'</code> <code>font_size</code> <code>int</code> <p>The requested font size in points.</p> <code>10</code> Source code in <code>fastnn/processors/cv/object_detection.py</code> <pre><code>@torch.no_grad()\ndef draw_bounding_boxes(\n    self,\n    image: torch.Tensor,\n    boxes: torch.Tensor,\n    labels: Optional[List[str]] = None,\n    colors: Optional[List[Union[str, Tuple[int, int, int]]]] = None,\n    width: int = 1,\n    font: Optional[str] = \"arial.ttf\",\n    font_size: int = 10,\n) -&gt; Tuple[torch.Tensor, np.array]:\n\n    \"\"\"\n    Added and modified from TorchVision utils.\n    Draws bounding boxes on given image.\n    The values of the input image should be uint8 between 0 and 255.\n    Args:\n        image (Tensor): Tensor of shape (C x H x W)\n        bboxes (Tensor): Tensor of size (N, 4) containing bounding boxes in (xmin, ymin, xmax, ymax) format. Note that\n            the boxes are absolute coordinates with respect to the image. In other words: `0 &lt;= xmin &lt; xmax &lt; W` and\n            `0 &lt;= ymin &lt; ymax &lt; H`.\n        labels (List[str]): List containing the labels of bounding boxes.\n        colors (List[Union[str, Tuple[int, int, int]]]): List containing the colors of bounding boxes. The colors can\n            be represented as `str` or `Tuple[int, int, int]`.\n        width (int): Width of bounding box.\n        font (str): A filename containing a TrueType font. If the file is not found in this filename, the loader may\n            also search in other directories, such as the `fonts/` directory on Windows or `/Library/Fonts/`,\n            `/System/Library/Fonts/` and `~/Library/Fonts/` on macOS.\n        font_size (int): The requested font size in points.\n    \"\"\"\n\n    if not isinstance(image, torch.Tensor):\n        raise TypeError(f\"Tensor expected, got {type(image)}\")\n    elif image.dtype != torch.uint8:\n        raise ValueError(f\"Tensor uint8 expected, got {image.dtype}\")\n    elif image.dim() != 3:\n        raise ValueError(\"Pass individual images, not batches\")\n\n    ndarr = image.permute(1, 2, 0).numpy()\n    img_to_draw = Image.fromarray(ndarr)\n\n    img_boxes = boxes.to(torch.int64).tolist()\n\n    draw = ImageDraw.Draw(img_to_draw)\n\n    pixel_ratio = max(1, (max(ndarr.shape[0], ndarr.shape[1]) // 1000))\n\n    for i, bbox in enumerate(img_boxes):\n        color = None if colors is None else colors[i]\n        draw.rectangle(bbox, width=width * pixel_ratio, outline=color)\n\n        if labels is not None:\n            txt_font = (\n                ImageFont.load_default()\n                if font is None\n                else ImageFont.truetype(font=font, size=font_size * pixel_ratio)\n            )\n            draw.text((bbox[0], bbox[1]), labels[i], fill=color, font=txt_font)\n\n    return torch.from_numpy(np.array(img_to_draw)).permute(2, 0, 1), np.array(\n        img_to_draw\n    )\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.cv.object_detection.ObjectDetectionProcessor.process","title":"<code>process(self, dir_path, transforms=ConvertImageDtype())</code>","text":"<p>Generate torch <code>Dataset</code> object from list of file paths or image Tensors. This provides clear tensor input representations for compatible models.</p> <p>Returns a Dataset</p> <ul> <li>dir_path - String path to directory of images you'd like to process</li> </ul> Source code in <code>fastnn/processors/cv/object_detection.py</code> <pre><code>def process(\n    self,\n    dir_path: str,\n    transforms: Optional[Callable] = ConvertImageDtype(torch.float),\n) -&gt; Dataset:\n    \"\"\"Generate torch `Dataset` object from list of file paths or image Tensors.\n    This provides clear tensor input representations for compatible models.\n\n    Returns a Dataset\n\n    * **dir_path** - String path to directory of images you'd like to process\n    \"\"\"\n\n    dataset = ImageDataset(root=dir_path, transforms=transforms)\n\n    return dataset\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.cv.object_detection.ObjectDetectionProcessor.process_batch","title":"<code>process_batch(self, dir_path, transforms=ConvertImageDtype(), mini_batch_size=8, use_gpu=False)</code>","text":"<p>Generate torch <code>Dataloader</code> object from data directory path. This provides clear tensor input representations for compatible models.</p> <p>Returns a <code>Dataloader</code></p> <ul> <li>dir_path - String path to directory of images you'd like to process</li> <li>mini_batch_size - Batch size for inference</li> <li>use_gpu - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu</li> </ul> Source code in <code>fastnn/processors/cv/object_detection.py</code> <pre><code>def process_batch(\n    self,\n    dir_path: str,\n    transforms: Optional[Callable] = ConvertImageDtype(torch.float),\n    mini_batch_size: int = 8,\n    use_gpu: bool = False,\n) -&gt; DataLoader:\n    \"\"\"Generate torch `Dataloader` object from data directory path.\n    This provides clear tensor input representations for compatible models.\n\n    Returns a `Dataloader`\n\n    * **dir_path** - String path to directory of images you'd like to process\n    * **mini_batch_size** - Batch size for inference\n    * **use_gpu** - Bool for using gpu or cpu. If set True but no gpu devices available, model will default to using cpu\n    \"\"\"\n\n    if use_gpu:\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        else:\n            logger.info(\"GPU not available\")\n            device = torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    dataset = self.process(dir_path=dir_path, transforms=transforms)\n\n    # Instead of a tensor batch, the lambda collate_fn will provide a list batch\n    dataloader = DataLoader(\n        dataset,\n        batch_size=mini_batch_size,\n        collate_fn=lambda x: [[t.to(device) for t in self._od_collate_fn(x)]],\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api-reference/processor.html#fastnn.processors.cv.object_detection.ObjectDetectionProcessor.process_output_batch","title":"<code>process_output_batch(self, outputs, dataset)</code>","text":"<p>Process output of object detection model into human legible results. Outputs from <code>FasterRCNNModule</code></p> <p>Returns batched results of list of list of tuples containing boxed images in tensor and numpy format</p> <ul> <li>outputs - List of batch output tensors from a model's forward pass</li> <li>dataset - Corresponding dataset with originial images matched with model outputs</li> </ul> Source code in <code>fastnn/processors/cv/object_detection.py</code> <pre><code>def process_output_batch(\n    self, outputs: List[List[torch.Tensor]], dataset: Dataset\n) -&gt; List[List[Tuple[torch.Tensor, np.array]]]:\n    \"\"\"Process output of object detection model into human legible results.\n    Outputs from `FasterRCNNModule`\n\n\n    Returns batched results of list of list of tuples containing boxed images in tensor and numpy format\n\n    * **outputs** - List of batch output tensors from a model's forward pass\n    * **dataset** - Corresponding dataset with originial images matched with model outputs\n\n    \"\"\"\n    # Labeled Images\n    results = []\n\n    for idx, out in enumerate(outputs):\n        labeled_images = []\n        for label_idx in range(1, len(out), 3):\n            labels = [self.label_strings[o] for o in out[label_idx]]\n\n            unique_labels = set(labels)\n            label_colors_map = {}\n            for label in unique_labels:\n                label_colors_map[label] = tuple(\n                    np.random.choice(range(256), size=3)\n                )\n\n            label_colors = [label_colors_map[label] for label in labels]\n\n            output_tensor, output_numpy = self.draw_bounding_boxes(\n                ConvertImageDtype(torch.uint8)(\n                    dataset[idx * (len(out) // 3) + label_idx // 3]\n                ),\n                out[label_idx - 1],\n                labels=labels,\n                colors=label_colors,\n            )\n            labeled_images.append((output_tensor, output_numpy))\n        results.append(labeled_images)\n\n    return results\n</code></pre>"}]}